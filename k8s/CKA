CKA
------

https://www.cncf.io/certification/cka/

https://github.com/cncf/curriculum

https://www.cncf.io/certification/candidate-handbook

Exam Tips: http://training.linuxfoundation.org/go//Important-Tips-CKA-CKAD

Use the code - KODEKLOUD20 - while registering for the CKA or CKAD exams at Linux Foundation to get a 20% discount.

https://docs.linuxfoundation.org/tc-docs/certification/faq-cka-ckad-cks

https://training.linuxfoundation.org/?s=kubernetes
https://docs.linuxfoundation.org/tc-docs/certification/tips-cka-and-ckad

https://github.com/sonyjames9/k8scurriculum

https://trainingportal.linuxfoundation.org/collections/kubernetes-developer
https://trainingportal.linuxfoundation.org/courses/certified-kubernetes-application-developer-ckad

https://www.cncf.io/certification/ckad/   developer
https://www.cncf.io/certification/cka/    administrator
https://training.linuxfoundation.org/skillcred/helm/

https://training.linuxfoundation.org/training/introduction-to-devops-and-site-reliability-engineering-lfs162/

https://training.play-with-kubernetes.com/kubernetes-workshop/
https://training.play-with-docker.com/alacart/


https://app.slack.com/client/TDSBA9B9V/C018VNH04LB/thread/C02LS58EGQ4-1687051526.410079   CodeKloud
https://app.slack.com/client/T09NY5SBT/C09NXKJKA/thread/C09NXKJKA-1688415137.305689       Kubernetes
https://kodekloud.com/pages/community

https://github.com/kodekloudhub/community-faq
https://github.com/kodekloudhub/community-faq#all-exams

https://opensource.com/article/20/7/tmux-cheat-sheet
https://vim.rtorr.com/
https://github.com/kodekloudhub/community-faq/blob/main/docs/etcd-faq.md


https://github.com/mmumshad/kubernetes-the-hard-way
https://github.com/kodekloudhub/certified-kubernetes-administrator-course 

https://kubernetes.io/docs/reference/kubectl/cheatsheet/

https://helm.sh/docs/intro/

https://kubernetes.io/docs/concepts/overview/kubernetes-api/

https://www.youtube.com/@kubesimplify/featured


NTT is hiring!
We NTT are looking for engineers who work in Open Source communities like containerd, Docker/Moby, Kubernetes, and their relevant projects. Visit https://www.rd.ntt/e/sic/recruit/ to see how to join us.

https://github.com/containerd/nerdctl
https://medium.com/nttlabs/nerdctl-359311b32d0e

https://github.com/containerd/containerd/blob/main/docs/getting-started.md
https://github.com/containerd/nerdctl

https://kubernetes.io/docs/reference/tools/map-crictl-dockercli/

crictl --runtime-endpoint
unix:///var/run/dockershim.sock
unix:///run/contianerd/containerd.sock
unix:///run/crio/crio.sock
unix:///var/run/cri-dockerd.sock

export CONTAINER_RUNTIME_ENDPOINT

ctr     contianerD
nerdctl containerD
crictl  k8s and all cri compatible runtimes


ETCD
-------
ETCDCTL is the CLI tool used to interact with ETCD.

ETCDCTL can interact with ETCD Server using 2 API versions - Version 2 and Version 3.  By default its set to use Version 2. Each version has different sets of commands.

For example ETCDCTL version 2 supports the following commands:

etcdctl backup
etcdctl cluster-health
etcdctl mk
etcdctl mkdir
etcdctl set


Whereas the commands are different in version 3

etcdctl snapshot save 
etcdctl endpoint health
etcdctl get
etcdctl put

To set the right version of API set the environment variable ETCDCTL_API command

export ETCDCTL_API=3



When API version is not set, it is assumed to be set to version 2. And version 3 commands listed above don't work. When API version is set to version 3, version 2 commands listed above don't work.



Apart from that, you must also specify path to certificate files so that ETCDCTL can authenticate to the ETCD API Server. The certificate files are available in the etcd-master at the following path. We discuss more about certificates in the security section of this course. So don't worry if this looks complex:

--cacert /etc/kubernetes/pki/etcd/ca.crt     
--cert /etc/kubernetes/pki/etcd/server.crt     
--key /etc/kubernetes/pki/etcd/server.key


So for the commands I showed in the previous video to work you must specify the ETCDCTL API version and path to certificate files. Below is the final form:



kubectl exec etcd-master -n kube-system -- sh -c "ETCDCTL_API=3 etcdctl get / --prefix --keys-only --limit=10 --cacert /etc/kubernetes/pki/etcd/ca.crt --cert /etc/kubernetes/pki/etcd/server.crt  --key /etc/kubernetes/pki/etcd/server.key" 



Kube API Scheduler

Kube Controller Manager

Kube Scheduler

Kubelet

KubeProxy	
process that runs on each node in k8s cluster, it looks for new services and everytime a svc is created it creates appropriate rules on each node to fwd traffic to those svcs to backend pod.One way is to use ip tables. It creates iptables rules on each node in cluster to forward traffic.


Pods.yaml:

apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
    type: front-end
spec:
  containers:
    - name: nginx-container
      image: nginx 

#Create/Spawn a k8s yaml object
kubectl create -f pod-def.yml
kubectl apply -f pod-def.yml

#Replace a k8s yaml object
kubectl replace -f pod-def.yml

kubectl get pods
kubectl describe pod pod-name

kubernetes labs
https://labs.play-with-k8s.com

alias k=kubectl
kubeadm init --apiserver-advertise-address $(hostname -i) --pod-network-cidr 10.5.0.0/16
kubectl apply -f https://raw.githubusercontent.com/cloudnativelabs/kube-router/master/daemonset/kubeadm-kuberouter.yaml
kubectl apply -f https://raw.githubusercontent.com/kubernetes/website/master/content/en/examples/application/nginx-app.yaml

mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config


Replication Controller -----

apiVersion: v1
kind: ReplicationController
metadata:
  name: myapp-rc
  labels: 
    app: myapp
    type: front-end  
spec:
  replicas: 2    
  template:
    metadata:
      name: myapp-pod
      labels:
        app: myapp
        type: front-end
    spec:
      containers:
        - name: nginx-container
          image: nginx 

Replica Set -----

apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: myapp-rs
  labels: 
    app: myapp
    type: front-end  
spec:
  replicas: 2    
  template:
    metadata:
      name: myapp-pod
      labels:
        app: myapp
        type: front-end
    spec:
      containers:
        - name: nginx-container
          image: nginx
  selector:
    matchLabels:
      type: front-end

# Update an existing RS, but this does not work
kubectl scale --replicas=6 -f rs.yaml

# Update by the replicas and replicaset name
kubectl scale --replicas=6  replicaset myapp-rs

k create -f rs.yml
k get rs
k delete rs myapp-rs
k replace -f rs.yaml


Deployment -----

apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-deployment
  labels: 
    app: myapp
    type: front-end  
spec:
  replicas: 2    
  template:
    metadata:
      name: myapp-pod
      labels:
        app: myapp
        type: front-end
    spec:
      containers:
        - name: nginx-container
          image: nginx
  selector:
    matchLabels:
      type: front-end


Service -----

apiVersion: v1
kind: Service
metadata:
  name: myapp-service
  labels: 
    app: myapp
    type: front-end  
spec:
  type: NodePort
  ports:
    - targetPort: 80
      port: 80
      NodePort: 30008
      selector:
        app: myapp
        type: front-end  
        
- if we dont provide targetPort, then port key value pair is considered as targetPort
- if NodePort is not provided, then a value between 30000 & 32767 is automatically allocated

k create -f svc-def.yaml
k get svc

curl http://ip:NodePort


ClusterIP
apiVersion: v1
kind: Service
metadata:
  name: backend
  labels:
    app: myapp
    type: front-end
spec:
  type: ClusterIP
  ports:
    - targetPort: 80
      port: 80
      selector:
        app: myapp
        type: front-end

LoadBalancer
apiVersion: v1
kind: Service
metadata:
  name: backend
  labels:
    app: myapp
    type: front-end
spec:
  type: LoadBalancer 
  ports:
    - targetPort: 80
      port: 80
      selector:
        app: myapp
        type: front-end


namespaces

db-svc.dev.svc.cluster.local
svc name.ns.svc.domain

db-service.dev.svc.cluster.local

Pods.yaml with ns:

apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  namespace: dev
  labels:
    app: myapp
    type: front-end
spec:
  containers:
    - name: nginx-container
      image: nginx

create ns
apiVersion: v1
kind: Namespace
metadata:
  name: myapp-pod

k create ns dev
k get pods --all-namespaces
k get pods -n default

k config set-context $(k config current-context) --namespace=dev

Compute-Quota.yaml -----
apiVersion: v1
kind: ResourceQuota
metadata:
  name: Compute-Quota.yaml
  namespace: dev
spec:
  hard:
   pods: "10"
   requests.cpu: "4"
   requests.memory: "5Gi"
   limits.cpu: "10"
   limits.memory: "10Gi"


Imperative and Declarative -----

the k8s commands is Imperative
Create Objects ---
k run --image=nginx nginx
k create deployment --image=nginx nginx
k expose deployment nginx --port 80
Update Objects ---
k edit deployment nginx
k scale deployment nginx --replicas=5
k set image deployment nginx nginx=nginx:1.18
k create -f nginx.yaml
k replace -f nginx.yaml
k delete -f nginx.yaml


the yaml definition is declarative, like how you create a pod, deployment, etc
k apply -f nginx.yaml

--dry-run: By default as soon as the command is run, the resource will be created. If you simply want to test your command , use the --dry-run=client option. This will not create the resource, instead, tell you whether the resource can be created and if your command is right.
-o yaml: This will output the resource definition in YAML format on screen.

Use the above two in combination to generate a resource definition file quickly, that you can then modify and create resources as required, instead of creating the files from scratch.

Another way to do this is to save the YAML definition to a file and modify

kubectl create deployment nginx --image=nginx --dry-run=client -o yaml > nginx-deployment.yaml
k create deployment webapp --image=kodekloud/webapp-color --replicas=3 --dry-run=client -o yaml > webapp-deployment.yaml
k create deployment webapp --image=kodekloud/webapp-color --replicas=3
k create deployment redis-deploy --image=redis --replicas=2 -n dev-ns

You can then update the YAML file with the replicas or any other field before creating the deployment.

Service
Create a Service named redis-service of type ClusterIP to expose pod redis on port 6379

kubectl expose pod redis --port=6379 --name redis-service --dry-run=client -o yaml
(This will automatically use the pod's labels as selectors)
Or
kubectl create service clusterip redis --tcp=6379:6379 --dry-run=client -o yaml (This will not use the pods labels as selectors, instead it will assume selectors as app=redis. You cannot pass in selectors as an option. So it does not work very well if your pod has a different label set. So generate the file and modify the selectors before creating the service)
https://github.com/kubernetes/kubernetes/issues/46191

Create a Service named nginx of type NodePort to expose pod nginx's port 80 on port 30080 on the nodes:

kubectl expose pod nginx --type=NodePort --port=80 --name=nginx-service --dry-run=client -o yaml
k expose pod redis --type=ClusterIP --port=6379 --name=redis-service
k expose pod httpd --type=ClusterIP --port=80 --target-port=80 --name=httpd

(This will automatically use the pod's labels as selectors, but you cannot specify the node port. You have to generate a definition file and then add the node port in manually before creating the service with the pod.)

Or

kubectl create service nodeport nginx --tcp=80:80 --node-port=30080 --dry-run=client -o yaml
k create service clusterip httpd --target-port=80 --dry-run=client -o yaml

(This will not use the pods labels as selectors)
Both the above commands have their own challenges. While one of it cannot accept a selector the other cannot accept a node port. I would recommend going with the kubectl expose command. If you need to specify a node port, generate a definition file using the same command and manually input the nodeport before creating the service.
https://github.com/kubernetes/kubernetes/issues/25478


https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands
https://kubernetes.io/docs/reference/kubectl/conventions/

# use imperative commands for creating a pod
k run --help
k run nginx-pod --image=nginx:alpine
k run redis --image=redis:alpine --labels="tier=db"

k create service clusterip --help
k create service clusterip redis-service --target-port=80 --dry-run=client -o yaml
k run custom-nginx --image=nginx --port=8080
k run httpd --image=httpd:alpine --port=80 --expose=true

k apply command ---
the apply command stores last applied changes in an annotation
annotations:
  kubectl.kubernetes.io/last-applied-configuration:
   {"json format of last applied configuration"}