Cluster Maintenance 
--------------------



























App Lifecycle Management
--------------------
k rollout status deployment/myapp-deployment
k rollout history deployment/myapp-deployment
k apply –f deployment-definition.yml
k set image deployment/myapp-deployment\nginx=nginx:1.9.1

k get rs
k rollout undo deployment/myapp-deployment
k run nginx --image=nginx


> kubectl create –f deployment-definition.yml
> kubectl get deployments
> kubectl apply –f deployment-definition.yml
> kubectl set image deployment/myapp-deployment nginx=nginx:1.9.1 
> kubectl rollout status deployment/myapp-deployment
> kubectl rollout history deployment/myapp-deployment
> kubectl rollout undo deployment/myapp-deployment

k set image deploy frontend simple-webapp=kodekloud/webapp-color:v2
k set image deploy frontend simple-webapp=kodekloud/webapp-color:v3

controlplane ~ ➜  cat curl-test.sh 
for i in {1..35}; do
   kubectl exec --namespace=kube-public curl -- sh -c 'test=`wget -qO- -T 2  http://webapp-service.default.svc.cluster.local:8080/info 2>&1` && echo "$test OK" || echo "Failed"';
   echo ""
done

controlplane ~ ➜  cat curl-pod.yaml 
apiVersion: v1 
kind: Pod
metadata:
  name: curl
  namespace: kube-public 
spec:
  containers:
  - image: byrnedo/alpine-curl:latest
    name: alpine-curl
    command: ["sleep", "5000"]


Commands Docker and k8s ------
--------------------

FROM Ubuntu
ENTRYPOINT ["sleep"]
CMD ["5"]
docker run ubuntu-sleeper 10
docker run --entrypoint sleep2.0 ubuntu-sleeper 10
docker run --name ubuntu-sleeper ubuntu-sleeper
docker run --name ubuntu-sleeper ubuntu-sleeper 10

apiVersion: v1
kind: Pod
metadata:
  name: ubuntu-sleeper-pod
spec:
  containers:
    - name: ubuntu-sleeper
      image: ubuntu-sleeper
      args: ["10"]

apiVersion: v1
kind: Pod
metadata:
  name: ubuntu-sleeper-pod
spec:
  containers:
    - name: ubuntu-sleeper
      image: ubuntu-sleeper
      command: ["sleep2.0"] #is same as docker ENTRYPOINT
      args: ["10"]          # is same as docker ENTRYPOINT

k create -f pod-fed.yaml

apiVersion: v1 
kind: Pod 
metadata:
  name: ubuntu-sleeper-2 
spec:
  containers:
  - name: ubuntu
    image: ubuntu
    command:
      - "sleep"
      - "5000"

FROM python:3.6-alpine
RUN pip install flask
COPY . /opt/
EXPOSE 8080
WORKDIR /opt
ENTRYPOINT ["python", "app.py"]
CMD ["--color", "red"]

controlplane ~ ➜  cat webapp-color-3/Dockerfile 
FROM python:3.6-alpine
RUN pip install flask
COPY . /opt/
EXPOSE 8080
WORKDIR /opt
ENTRYPOINT ["python", "app.py"]
CMD ["--color", "red"]

controlplane ~ ➜  cat webapp-color-3/webapp-color-pod-2.yaml 
apiVersion: v1 
kind: Pod 
metadata:
  name: webapp-green
  labels:
      name: webapp-green 
spec:
  containers:
  - name: simple-webapp
    image: kodekloud/webapp-color
    command: ["python", "app.py"]
    args: ["--color", "pink"]

k run webapp-green --image=kodekloud/webapp-color -- --color green
k run webapp-green --image=kodekloud/webapp-color --command -- python app2.py -- --color green
k replace --force -f /tmp/k-edit-file.yaml


ENV VARS
--------------------
1. Plain key value
2. ConfigMap
3. Secrets

docker run -e APP_COLOR=pink simple-webapp-color

apiVersion: v1 
kind: Pod 
metadata:
  name: simple-webapp-color
spec:
  containers:
  - name: simple-webapp-color
    image: simple-webapp-color
    ports:
      - containerPort: 8080
    env:
      - name: APP_COLOR
        value: pink

#Configmaps
    env:
      - name: APP_COLOR
        valueFrom:
            configMapKeyRef: 

#Secrets
    env:
      - name: APP_COLOR
        valueFrom:
            secretKeyRef: 

apiVersion: v1 
kind: Pod 
metadata:
  name: simple-webapp-color
spec:
  containers:
  - name: simple-webapp-color
    image: simple-webapp-color
    ports:
      - containerPort: 8080
    envFrom:
      - configMapKeyRef:
          name: app-config
          key: APP_COLOR

apiVersion: v1 
kind: Pod 
metadata:
  name: webapp-color
  labels:
    name: webapp-color
spec:
  containers:
  - name: webapp-color
    image: kodekloud/webapp-color
    envFrom:
      - configMapRef:
          name: webapp-config-map


2 Phases in creating config map
1. Create ConfigMap
2. Inject the configMap into Pod

1. k create configmap    Imperative
2. k create -f           Declarative

k create configmap \
  app-config --from-literal=APP_COLOR=blue

k create configmap \
  app-config2 --from-literal=APP_COLOR=darkblue \ 
  -from-literal=APP_OTHER=disregard

k create configmap <config-name> --from-file=<path-to-file>
k create configmap \
  app-config --from-file=app_config.properties

configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
data:
  APP_COLOR: blue
  APP_MODE: prod

k create -f configmap.yaml


app-config:
APP_COLOR: blue
APP_MODE: prod

mysql-config:
port: 3306
max_allowed_packet: 128M

redis-config:
port: 6379
rdb-compression: yes

k get configmaps
k describe configmaps

apiVersion: v1 
kind: Pod 
metadata:
  name: simple-webapp-color
spec:
  containers:
  - name: simple-webapp-color
    image: simple-webapp-color
    ports:
      - containerPort: 8080
    envFrom:
      - configMapKeyRef:
          name: app-config

spec:
  containers:
    - name: test-container
      image: gcr.io/google_containers/busybox
      command: ["/bin/sh", "-c", "env"]
      env:
        - name: SPECIAL_LEVEL_KEY
          valueFrom:
            configMapKeyRef:
              name: a-config
              key: akey

spec:
  containers:
    - name: test-container
      image: registry.k8s.io/busybox
      command: [ "/bin/sh", "-c", "env" ]
      envFrom:
      - configMapRef:
          name: special-config

spec:
  containers:
    - name: test-container
      image: registry.k8s.io/busybox
      command: [ "/bin/sh", "-c", "env" ]
      env:
        - name: SPECIAL_LEVEL_KEY
          valueFrom:
            configMapKeyRef:
              name: special-config
              key: special.how
        - name: LOG_LEVEL
          valueFrom:
            configMapKeyRef:
              name: env-config
              key: log_level


k get pods -A --no-headers | wc -l 

https://kubernetes.io/docs/concepts/configuration/configmap/
https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/


Secrets --------------------
Secrets are used to store password or keys(sensitive information). They are stored in encoded base64 format.
1. Create Secret
2. Inject Secret

Imperative --------
k create secret generic
k create secret generic <secret-name> --form-literal=<key>=<value>
k create secret generic \
app-secret --form-literal=DB_Host=MYSQL \
           --form-literal=DB_User=root \
           --form-literal=DB_Password=paswrd
k create secret generic <secret-name> --from-file=<path-to-file>
k create secret generic \
app-secret --from-file=app-secret.properties

Declarative --------
k create -f secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: app-secret
data:
  DB_Host: bXlzcWw=
  DB_User: cm9vdA==
  DB_Password: cGFzd3Jk

echo -n 'mysql' | base64
bXlzcWw=
echo -n 'root' | base64
cm9vdA==
echo -n 'paswrd' | base64
cGFzd3Jk

k get secrets
k desscribe secrets
k get secrets app-secret -oyaml

echo -n 'bXlzcWw=' | base64 --decode
mysql
echo -n 'cm9vdA==' | base64 --decode
root
echo -n 'cGFzd3Jk' | base64 --decode
paswrd

apiVersion: v1
kind: Pod
metadata:
  name: nginx
  namespace: dev
  labels:
    app: nginx
spec:
  containers:
    - name: nginx-container
      image: nginx
      ports:
        - containerPort: 8080
      envFrom:
        - secretRef:
            name: app-secret

k create -f pod-def.yaml

SingleEnv ------
env:
  - name: DB_Password
    valueFrom:
      secretKeyRef:
        name: app-secret
        key: DB_Password

Volume -------
volumes:
- name: app-secret-volume
  secret:
    secretName: app-secret

If you check inside volumes inside container you can find secrets at this location

ls /opt/app-secret-volumes
DB_Host         DB_Password         DB_User

cat /opt/app-secret-volumes/DB_Password
paswrd

-- Secrets are not encrypted, only encoded.
-- Do not checkin secret objects to SCM along with code
-- Secrets are not encrypted in ETCD, consider enablling encryption at REST.
https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/

-- Anyone able to create pods/deployments in the same namespace can access the secrets.
-- Consider least privilege access to Secrets - RBAC.
-- Consider 3rd party secret store providers like AWS Provider, Azure Provider, GCP Provider, Vault Provider -- this way the secrets are not stored in etcd but external secret provider those providers take care of most of the security.

https://kubernetes.io/docs/concepts/configuration/secret/#protections
https://kubernetes.io/docs/concepts/configuration/secret/#risks

https://www.vaultproject.io/
Having said that, there are other better ways of handling sensitive data like passwords in Kubernetes, such as using tools like Helm Secrets, HashiCorp Vault. I hope to make a lecture on these in the future.


kubeadm init --apiserver-advertise-address $(hostname -i) --pod-network-cidr 10.5.0.0/16
kubectl apply -f https://raw.githubusercontent.com/cloudnativelabs/kube-router/master/daemonset/kubeadm-kuberouter.yaml
kubectl apply -f https://raw.githubusercontent.com/kubernetes/website/master/content/en/examples/application/nginx-app.yaml
alias k=kubectl
k taint node node1 node-role.kubernetes.io/control-plane:NoSchedule-
yum install wget -y
wget https://go.dev/dl/go1.21.0.linux-amd64.tar.gz
tar -C /usr/local -xzf go1.21.0.linux-amd64.tar.gz
export PATH=$PATH:/usr/local/go/bin                                   
go version
go install github.com/coreos/etcd/etcdctl@latest
cd etcd/bin


ETCDCTL_API=3 ./etcdctl --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key get /registry/secrets/default/

[node1 bin]$ ETCDCTL_API=3 ./etcdctl --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key get /registry/secrets/default/my-sec

/etc/kubernetes/manifests/kube-apiserver.yaml


k taint node node1 node-role.kubernetes.io/control-plane:NoSchedule-  #Remove node taint if present on a node

k create secret generic my-sec --from-literal=key1=secretkeylol

encrypt.yaml
apiVersion: apiserver.config.k8s.io/v1
kind: EncryptionConfiguration
resources:
  - resources:
      - secrets
    providers:
      - aescbc:
          keys:
            - name: key1
              # See the following text for more details about the secret value
              secret: aBRN2uGzTsrN3XSeXgXM5OImPNMKgg9DKfSYzzxzADA=
      - identity: {}
[node1 bin]$ mkdir /etc/kubernetes/enc
[node1 bin]$ mv encrypt.yaml /etc/kubernetes/enc                                     
[node1 bin]$ ls /etc/kubernetes/enc
encrypt.yaml

[node1 bin]$ vi /etc/kubernetes/manifests/kube-apiserver.yaml

spec:
  containers:
  - command:
    - kube-apiserver
    - --advertise-address=192.168.0.8
    ...
    - --encryption-provider-config=/etc/kubernetes/enc/encrypt.yaml
    volumeMounts:             # MOUNTING OF POD VOLUME
    ...
    - name: enc                           # add this line
      mountPath: /etc/kubernetes/enc      # add this line
      readonly: true
  volumes:                    # MOUNTING OF LOCAL CLUSTER VOLUME
  ...
  - name: enc                             # add this line
    hostPath:                             # add this line
      path: /etc/kubernetes/enc           # add this line
      type: DirectoryOrCreate

kube-apiserver will restart
k create secret generic my-sec2 --from-literal=key1=secretkeylol

Now new secret keys will be encrypted
[node1 bin]$ ETCDCTL_API=3 ./etcdctl --cacert=/etc/kubernetes/pki/etcd/ca.crt --cer
etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key get /r
stry/secrets/default/my-sec2 | hexdump -C

Ensure older secrets are also encrypted after above secret encryption steps
kubectl get secrets --all-namespaces -o json | kubectl replace -f -
https://chromium.googlesource.com/external/github.com/coreos/etcd/+/release-3.0/etcdctl/README.md

MULTI CONTAINER Pod --------------------
apiVersion: v1
kind: Pod
metadata:
  name: nginx
  namespace: dev
  labels:
    app: nginx
spec:
  containers:
    - name: nginx-container
      image: nginx
      ports:
        - containerPort: 8080
    - name: log-agent
      image: log-agent
  

k run yellow --image=busybox --dry-run=client -o yaml
k -n elastic-stack exec -it app --  /log/app.log

apiVersion: v1
kind: Pod
metadata:
  labels:
    name: app
  name: app
  namespace: elastic-stack
spec:
  containers:
  - image: kodekloud/event-simulator
    name: app
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /log
      name: log-volume
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: kube-api-access-h68qm
      readOnly: true
  - image: kodekloud/filebeat-configured
    name: sidecar
    volumeMounts:
    - mountPath: /var/log/event-simulator/ #LOGS ARE READ AND SENT TO ELASTICSEARCG
      name: log-volume

MULTI CONTAINER PODS DESIGN PATTERN
1. Sidecar
2. Adapter
3. Ambassador


INIT CONTAINER --------------------
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
spec:
  containers:
  - name: myapp-container
    image: busybox:1.28
    command: ['sh', '-c', 'echo The app is running! && sleep 3600']
  initContainers:
  - name: init-myservice
    image: busybox:1.28
    command: ['sh', '-c', 'until nslookup myservice; do echo waiting for myservice; sleep 2; done;']
  - name: init-mydb
    image: busybox:1.28
    command: ['sh', '-c', 'until nslookup mydb; do echo waiting for mydb; sleep 2; done;']

https://kubernetes.io/docs/concepts/workloads/pods/init-containers/

SELF HEALING APP
Kubernetes supports self-healing applications through ReplicaSets and Replication Controllers. The replication controller helps in ensuring that a POD is re-created automatically when the application within the POD crashes. It helps in ensuring enough replicas of the application are running at all times.
Kubernetes provides additional support to check the health of applications running within PODs and take necessary actions through Liveness and Readiness Probes. However these are not required for the CKA exam and as such they are not covered here. These are topics for the Certified Kubernetes Application Developers (CKAD) exam and are covered in the CKAD course.

Logging and Monitor
--------------------
https://kubernetes.io/docs/tasks/debug/debug-cluster/resource-usage-monitoring/

https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/

kubelet is responsible for receiving instructions from the k8s API master server and runnng pods on the nodes.
Kubelet also contains a sub component known as CAdvisor/Container Advisor.
Cadvisor is responsible for retrieving performance metrics from pods and exposing them through the kubelet API to make the metrics available for the Metrics Server. 
> minicube addons enable metrics-server
> git clone https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/
> k create -f deploy/1.27+/
> k top node
> k top pod

> git clone https://github.com/kodekloudhub/kubernetes-metrics-server.git

k create -f event-sim.yaml
apiVersion: v1
kind: Pod
metadata:
  name: event-simulator-pod
  labels:
spec:
  containers:
    - name: event-simulator
      image: kodekloud/event-simulator

k logs -f event-simulator-pod







Scheduler
------
https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/

apiVersion: v1
kind: Pod
metadata:
  name: nginx
  namespace: dev
  labels:
    app: nginx
spec:
  containers:
    - name: nginx-container
      image: nginx
      ports:
        - containerPort: 8080
  nodeName: node02


For already existing pod, if you need to assign the pod to a node, then you will need to apply below binding object yaml to a pod. Send a POST request to the pods binding data.

apiVersion: v1
kind: Binding
metadata:
  name: nginx
target:
  apiVersion: v1
  kind: Node
  name: node2

curl --header "Content-Type:application/json" --request POST --data 'above yaml file' http://$$SERVER/api/v1/namespaces/default/pods/$PODNAME/binding/

k describe pod nginx - the node is empty
k get pods -n kube-system - you should see the scheduler pod
k replace --force -f nginx.yaml
k get pods -owide

LABELS -------
k get pods -l env=prod,bu=finance --no-headers
k get pods -l env=prod,bu=finance --no-headers | wc -l
k get all -l env=prod,bu=finance --no-headers | wc -l

TAINTS AND TOLERATIONS -------

k taint nodes node-name key=value:taint-effect
No schedule | PreferNoSchedule | NoExecute - evicted

NoSchedule - Pods will not be scheduled on the nodes
PreferNoSchedule - system will try to avoid placing the pod, but not guaranteed
NoExecute - new pods will not be scheduled and existing ones are evicted 

k taint nodes node1 app=blue:NoSchedule
k taint nodes node1 app=blue:NoSchedule-
k describe node kubemaster | grep Taint
k describe node controlplane | grep -i Taint

k taint nodes node01 spray=mortein:NoSchedule
k taint nodes controlplane node-role.kubernetes.io/control-plane:NoSchedule-

k run mosquito --image=nginx --port=8080

apiVersion: v1
kind: Pod
metadata:
  labels:
    name: bee
  name: bee
spec:
  containers:
    - name: nginx-container
      image: 
  restartPolicy: Always
  dnsPolicy: ClusterFirst
  tolerations:
  - key: "spray"
    operator: "Equal"
    value: "mortein"
    effect: NoSchedule

k run bee --image=nginx --dry-run=client -o yaml > bee.yaml


Node selector
k label nodes <node-name> <key>=<value>
k label nodes node-01 size=Large

apiVersion: v1
kind: Pod
metadata:
  labels:
    name: bee
  name: bee
spec:
  containers:
    - name: nginx-container
      image: nginx
  nodeSelector:
    size: Large

apiVersion: v1
kind: Pod
metadata:
  labels:
    name: bee
  name: bee
spec:
  containers:
    - name: nginx-container
      image: nginx
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: size
            operator: In
            values:
            - Large
            - Medium

operator: In, NotIn, Exists

NodeAffinityTypes
available - 
requiredDuringSchedulingIgnoredDuringExecution:
preferredDuringSchedulingIgnoredDuringExecution:

planned - 
requiredDuringSchedulingRequiredDuringExecution:


apiVersion: apps/v1
kind: Deployment
metadata:
  name: blue
  labels:
    color: blue
spec:
  replicas: 3
  template:
    metadata:
      name: blue
      labels:
        color: blue
    spec:
      containers:
        - name: nginx-container
          image: nginx
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: color
                operator: In
                values:
                - blue
  selector:
    matchLabels:
      color: blue

# k create deployment red --image=nginx --replicas=2 --dry-run=client -oyaml > red2.yaml
# The Deployment "red" is invalid: spec.template.spec.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.
# nodeSelectorTerms[0].matchExpressions[0].values: Forbidden: may not be specified when `operator` is 'Exists' or 'DoesNotExist'

---
apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: red
  name: red
spec:
  replicas: 2
  selector:
    matchLabels:
      app: red
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: red
    spec:
      containers:
      - image: nginx
        name: nginx
        resources: {}
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: node-role.kubernetes.io/control-plane
                operator: Exists
status: {}

Resource Requests --------

apiVersion: v1
kind: Pod
metadata:
  labels:
    name: bee
  name: bee
spec:
  containers:
  - name: nginx-container
    image: nginx
    resources:
      requests:
        memory: "2Gi"
        cpu: 2
      limits:
        memory: "2Gi"
        cpu: 2


CPU ----
0.1 CPU <-> 100M
1M

1 CPU
1 AWS VCPU
1 GCP Core
1 Azure Core
1 Hyperthrea


MEMORY -----
256Mi = 268435456
268M
1G    Gigabyte
1M
1K
1Gi   Gibibyte
1Mi   Mebibyte
1Ki   Kibibyte
 
CPU ----
NO REQUESTS
NO LIMITS

NO REQUESTS
LIMITS

REQUESTS
LIMITS

This is the right choice in cpu selection
REQUESTS
NO LIMITS

MEMORY ----
NO REQUESTS
NO LIMITS

NO REQUESTS
LIMITS

REQUESTS
LIMITS

REQUESTS
NO LIMITS


LimitRange -----
CPU---
apiVersion: v1
kind: LimitRange
metadata:
  name: cpu-resource-constraint
spec:
  limits:
  - default:
      cpu: 500m
    defaultRequest:
      cpu: 500m
    max:
      cpu: "1"
    min:
      cpu: 100m
    type: Container

MEMORY---
apiVersion: v1
kind: LimitRange
metadata:
  name: memory-resource-constraint
spec:
  limits:
  - default:
      memory: 1Gi
    defaultRequest:
      memory: 1Gi
    max:
      memory: 1Gi
    min:
      memory: 500Mi
    type: Container

RESORCE QUOTA ---
created at namespace level

apiVersion: v1
kind: ResourceQuota
metadata:
  name: my-resource-quota
spec:
  hard:
    requests.cpu: 4
    requests.memory: 4Gi
    limits.cpu: 10
    limits.memory: 10Gi

which will give the complete spec with node name, or:
kubectl get nodes -o json | jq '.items[].spec'
kubectl get nodes -o json | jq '.items[].spec.taints'


DAEMON SETS ---
A pod in every node, this is used in logging and monitoring events in a cluster.
kube-proxy can be deployed as a daemonset in cluster.
weave-net deploys a networking pod in each cluster.

apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: monitoring-daemon
  labels: 
    app: monitoring-agent
spec:
  selector:
    matchLabels:
      app: monitoring-agent
  template:
    metadata:
      labels:
        app: monitoring-agent
    spec:
      containers:
        - name: monitoring-agent
          image: monitoring-agent

DS uses default scheduler and node affinity rules to schedule pods on nodes





Static pod Busybox
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: static-busybox
  name: static-busybox
  namespace: default
spec:
  containers:
  - image: busybox
    imagePullPolicy: Always
    name: busybox
    command: ["/bin/sh", "-ec", "sleep 1000"]

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: static-busybox
  name: static-busybox
spec:
  containers:
  - command:
    - sleep
    - "1000"
    image: busybox
    name: static-busybox
  restartPolicy: Never

Multiple Scheduler ------

apiVersion: kubescheduler.config.k8s.io/v1
kind: KubeSchedulerConfiguration
profiles:
- schedulerName: my-scheduler
leaderElection:
  leaderElect: true
  resourceNamespace: kube-system
  resourceName: lock-object-my-scheduler

my-custom-scheduler.yaml
apiVersion: v1
kind: Pod
metadata:
  name: my-custom-scheduler
  namespace: kube-system
spec:
  containers:
  - image: busybox
    name: busybox
    command:
    - kube-scheduler
    - --address=127.0.0.1
    - --kubeconfig=/etc/kubernetes/scheduler.conf
    - --config=/etc/kubernetes/my-scheduler-config.yaml
k get events -owide
k logs custom-scheduler -n=kube-system

k create configmap my-scheduler-config --from-file=/root/my-scheduler-config.yaml -n kube-system


controlplane ~ ➜  cat my-scheduler-config.yaml 
apiVersion: kubescheduler.config.k8s.io/v1beta2
kind: KubeSchedulerConfiguration
profiles:
  - schedulerName: my-scheduler
leaderElection:
  leaderElect: false

controlplane ~ ➜  cat nginx-pod.yaml 
apiVersion: v1 
kind: Pod 
metadata:
  name: nginx 
spec:
  schedulerName: my-scheduler
  containers:
  - image: nginx
    name: nginx

controlplane ~ ➜  cat my-scheduler-configmap.yaml
apiVersion: v1
data:
  my-scheduler-config.yaml: |
    apiVersion: kubescheduler.config.k8s.io/v1beta2
    kind: KubeSchedulerConfiguration
    profiles:
      - schedulerName: my-scheduler
    leaderElection:
      leaderElect: false
kind: ConfigMap
metadata:
  creationTimestamp: null
  name: my-scheduler-config
  namespace: kube-system

controlplane ~ ➜  cat my-scheduler.yaml 
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: my-scheduler
  name: my-scheduler
  namespace: kube-system
spec:
  serviceAccountName: my-scheduler
  containers:
  - command:
    - /usr/local/bin/kube-scheduler
    - --config=/etc/kubernetes/my-scheduler/my-scheduler-config.yaml
    image: registry.k8s.io/kube-scheduler:v1.27.0
    livenessProbe:
      httpGet:
        path: /healthz
        port: 10259
        scheme: HTTPS
      initialDelaySeconds: 15
    name: kube-second-scheduler
    readinessProbe:
      httpGet:
        path: /healthz
        port: 10259
        scheme: HTTPS
    resources:
      requests:
        cpu: '0.1'
    securityContext:
      privileged: false
    volumeMounts:
      - name: config-volume
        mountPath: /etc/kubernetes/my-scheduler
  hostNetwork: false
  hostPID: false
  volumes:
    - name: config-volume
      configMap:
        name: my-scheduler-config


Scheduler Profiles -----
Scheduling Queue -> Filtering -> Scoring -> Binding
Scheduling Queue(PrioritySort) -> Filtering(NodeResourcesFit,NodeName,NodeUnschedulable) -> Scoring(NodeResourcesFit, ImageLocality) -> Binding(DefaultBinder)
- Extension Point can be plugged to the scheduling plugins
- Scheduling Queue(queue sort) -> Filtering(preFilter,filter,postFilter) -> Scoring(preScore,score,reserve,permit) -> Binding(preBind,bind,postBind)
apiVersion: v1 
kind: Pod 
metadata:
  name: nginx 
spec:
  priorityClassName: high-priority
  containers:
  - image: nginx
    name: nginx

Sample Multiple Scheduler Profiles -----
apiVersion: kubescheduler.config.k8s.io/v1
kind: KubeSchedulerConfiguration
profiles:
- schedulerName: my-scheduler-2
  plugins:
    score:
      disabled:
        - name: TaintToleration
      enabled:
        - name: CustomPluginA
        - name: CustomPluginB
- schedulerName: my-scheduler-3
  plugins:
    preScore:
      disabled:
        - name: '*'
    score:
      disabled:
        - name: '*'
- schedulerName: my-scheduler-4

https://kubernetes.io/docs/tasks/extend-kubernetes/configure-multiple-schedulers/

Scheduler code hierarchy overview
https://github.com/kubernetes/community/blob/master/contributors/devel/sig-scheduling/scheduling_code_hierarchy_overview.md

Advance Scheduling in K8s ------
https://kubernetes.io/blog/2017/03/advanced-scheduling-in-kubernetes/

How does Kubernetes' scheduler work?
https://jvns.ca/blog/2017/07/27/how-does-the-kubernetes-scheduler-work/
https://stackoverflow.com/questions/28857993/how-does-kubernetes-scheduler-work