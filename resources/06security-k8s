------------------------------------
SECURITY IN K8S START
------------------------------------

kube-apiserver - center of all operations of kubernetes; we interact with it thru kube control utility or by accessing the API directly  --- FIRST LINE OF DEFENCE
1. Who can access? the API is defined by the authentication mechanisms.
   1. Static Password Files -- Username and Passwords
   2. Static Token Files -- Username and Tokens
   3. Certificates
   4. External Authentication providers - LDAP
   5. Service Accounts -- Created for machines
2. What can they do?
   1. RBAC Authorization - Role Based Access Control -- what role is assigned to users associated to groups with specific permissions
   2. ABAC Authorization - Attribute Based Access Control
   3. Node Authorization
   4. Webhook Mode

All communication to KubeApiServer from KubeProxy, KubeScheduler, KubeControllerManager, Kubelet, ETCDCluster is via TLS Certificates

NW Policies - Restrict access from one pod to another

Accounts ---

Users --
Admins      kubectl
Developers  curl https://kube-server-ip:6443

Service Accounts --
Bots
 k create serviceaccount sa1
 k get sa



1. Static Password Files -- Username and Passwords
  user-details.csv
  password123,user1,u001,group1
  password123,user2,u002,group1
  password123,user3,u003,group2
   1. kube-apiserver --basic-auth-file=user-details.csv
   2. Add this paremeter in kube-apiserver.service
   3. Restart the service
   4. *** If you use kubeadm for setup, then you must modify the kube-apiserver.yaml pod definition file and kubeadm tool will automatically restart the kube-apiserver once you update the file
   5. In the /etc/kubernetes/manifests/kube-apiserver.yaml, under command, you should append
   - --basic-auth-file=user-details.csv

  Authenticate User --- 
  curl -v -k https://master-node-ip:6443/api/v1/pods -u "user1:password123"


2. Static Token Files -- Username and Tokens
  user-tokens.csv
  token123,user1,u001,group1
  token123,user2,u002,group1
  password123,user3,u003,group2
   1. kube-apiserver --token-auth-file=user-token.csv
   2. Add this paremeter in kube-apiserver.service
   3. Restart the service
   4. *** If you use kubeadm for setup, then you must modify the kube-apiserver.yaml pod definition file and kubeadm tool will automatically restart the kube-apiserver once you update the file
   5. In the /etc/kubernetes/manifests/kube-apiserver.yaml, under command, you should append
   - --token-auth-file=user-tokens.csv
  Authenticate User --- 
  curl -v -k https://master-node-ip:6443/api/v1/pods --header "Authorization: Bearer token"

Note: 
1. This is not a recommended authentication mechanism
2. Consider volume mount while providing the auth file in a kubeadm setup
3. Setup Role Based Authorization for the new users

TLS Basics ----







































kube-apiserver
- --authorization-mode=Node,RBAC
- --advertise-address=172.17.0.18
- --allow-privileged=true
- --client-ca-file=/etc/kubernetes/pki/ca.crt
- --disable-admission-plugins=PersistentVolumeLabel
- --enable-admission-plugins=NodeRestriction
- --enable-bootstrap-token-auth=true
- --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
- --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
- --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
- --etcd-servers=https://127.0.0.1:2379
- --insecure-port=0
- --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt - --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
- --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
- --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
- --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
- --requestheader-allowed-names=front-proxy-client
- --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
- --requestheader-extra-headers-prefix=X-Remote-Extra-
- --requestheader-group-headers=X-Remote-Group
- --requestheader-username-headers=X-Remote-User
- --secure-port=6443
- --service-account-key-file=/etc/kubernetes/pki/sa.pub
- --service-cluster-ip-range=10.96.0.0/12
- --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
- --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
 
 openssl genrsa -out old-ca.key 2048
openssl req -new -key old-ca.key -subj "/CN=old-ca" -out old-ca.csr
openssl x509 -req -in old-ca.csr -signkey old-ca.key -out old-ca.crt -days 365
openssl x509 -req -in ca.csr -signkey ca.key -out server.crt -days 365
openssl req -new -key apiserver-kubelet-client.key -out apiserver-kubelet-client.csr -subj "/CN=kube-apiserver-kubelet-client/O=system:masters"
openssl req -new -key apiserver-kubelet-client.key -out apiserver-kubelet-client.csr -subj "/CN=kube-apiserver-kubelet-client/O=system:masters"
openssl x509 -req -in apiserver-kubelet-client.csr -CA /root/new-ca/old-ca.crt -CAkey /root/new-ca/old-ca.key -CAcreateserial -out apiserver-kubelet-client-new.crt -days 365
openssl req -new -key apiserver-etcd-client.key -out apiserver-etcd-client.csr -subj "/CN=kube-apiserver-etcd-client/O=system:masters"
openssl x509 -req -in apiserver-etcd-client.csr -CA /root/new-ca/old-ca.crt -CAkey /root/new-ca/old-ca.key -CAcreateserial -out apiserver-etcd-client-new.crt -days 365
openssl req -new -key apiserver-etcd-client.key -out apiserver-etcd-client.csr -subj "/CN=kube-apiserver-etcd-client/O=system:masters"
openssl x509 -req -in apiserver-etcd-client.csr -CA /root/new-ca/old-ca.crt -CAkey /root/new-ca/old-ca.key -CAcreateserial -out apiserver-etcd-client-new.crt -days 365
openssl req -new -key /etc/kubernetes/pki/apiserver-etcd-client.key -out apiserver-etcd-client.csr -subj "/CN=kube-apiserver-etcd-client/O=system:masters" openssl x509 -req -in apiserver-etcd-client.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out apiserver-etcd-client.crt -days -10
openssl x509 -req -in apiserver-etcd-client.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out apiserver-etcd-client.crt -startdate 190101010101Z 20170101000000Z
200801010000Z
"openssl", "req", "-new", "-key" ,"/etc/kubernetes/pki/apiserver-etcd-client.key", "-out", "/etc/kubernetes/pki/apiserver-etcd-client.csr", "-subj", "/CN=kube-apiserver-etcd- client/O=system:masters"
"openssl", "x509", "-req", "-in", "/etc/kubernetes/pki/apiserver-etcd-client.csr", "-CA", "/etc/kubernetes/pki/etcd/ca.crt", "-CAkey", "/etc/kubernetes/pki/etcd/ca.key", "-CAcreateserial", "-out", "/etc/kubernetes/pki/apiserver-etcd-client.crt"
openssl x509 -req -in /etc/kubernetes/pki/apiserver-etcd-client.csr -CA /etc/kubernetes/pki/etcd/ca.crt -CAkey /etc/kubernetes/pki/etcd/ca.key -CAcreateserial -out /etc/kubernetes/pki/apiserver-etcd-client.crt -days 100
 openssl x509 -req -in apiserver.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out apiserver.crts





























SECURITY IN K8S END
------------------------------------




Cluster Maintenance 
--------------------
k drain node01   -- drain pods onto another node, pods are gracefully terminated and recreated on another node
- Node is also cordoned or marked as unschedulable, no pods can be scheduled until the restriction is removed specifically.
Once all pods are moved to another node, the node from which all pods were drained can be reboot or updated as required.
When the node comes back online, the node is still unschedulable, the node needs to be uncordon so pods can be scheduled on it again.

k drain node01
k cordon node02
k uncordon node01
k drain node01 --ignore-daemonsets
k get pods --field-selector spec.nodeName=node01
k get pods -owide
kubectl get pods -o wide --show-labels

Pods not managed by RC,RS,Job,DS, SS will get deleted when node is drained.
RC,RS,Job,DS, SS will ensure pods are recreated on other nodes

API Version - k8s version --------------------
- v1.11.3 major.minor.patch
kubernetes releases pages
executables in tar.gz in each release

The Kubernetes API https://kubernetes.io/docs/concepts/overview/kubernetes-api/
Client libraires https://kubernetes.io/docs/reference/using-api/client-libraries/
API Conventions https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md
Changing the API https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api_changes.md


Cluster Update Process --------------------
controlplane components ----
1. kube-apiserver         x         v1.10
2. controller-manager     x-1       v1.9/10
3. kube-scheduler         x-1       v1.9/10
4. kubelet                x-2       v1.8/9/10
5. kube-proxy             x-2       v1.8/9/10
6. kubectl                x+1 > x-1 v1.11/10/9

7. etcd cluster
8. coreDNS

Recent 3 minor versions are supported. 1.10/11/12

k upgrade plan
k upgrade apply

First master nodes and then worker nodes
apiserver/scheduler/controller-manager goes down briefly in master nodes. During this time
1. no new deployment/modifictaions takes place
2. controller-manager dont function either, deleted pod is not respawned
3. worker and applications are not impacted
4. No access via kubectl/k8s api
5. Users will not be impacted

The master node is updated to latest and then its time for worker nodes to update.
1. Upgrade all nodes at same time - there is significant downtime in this approach, no apps, deployments or users. Only after the update all pods, resources are in working condition
2. Upgrade nodes one at a time, workload and resources are moved to other nodes meantime. Once node1 is backup with upgraded k8s version; the workloads and resources are moved back to worker node01. This is performed for all nodes
3. Add new nodes with newer software version to cluster, this is live working scenario in cloud env where you can easily provision new nodes and decomission old ones. Workloads will be moved to newer node, this process continues until every node workloads and resources are migrated to new node version


Upgrade 1.11 to 1.13
kubeadm upgrade plan
1.11 -> 1.12 -> 1.13
apt-get upgrade -y kubernetes=1.12.0-00
k upgrade apply v1.12.0
k get nodes
master  READY   MASTER  V1.11.3
node01  READY   NONE    V1.11.3


ALWAYS Upgrade kubelets MANUALLY
apt-get upgrade -y kubernetes=1.12.0-00
systemctl restart kubelet
k get nodes
master  READY   MASTER  V1.12.0
node01  READY   NONE    V1.11.3

k drain node01
apt-get upgrade -y kubernetes=1.12.0-00
# Upgrade kubeadm and kubelet packages like in master node
kubeadm upgrade node config --kubelet-version v1.12.0
systemctl restart kubelet
k uncordon node01

node was unschedulable and after marking it as schedulable, the pods/resources/workloads can only come back to the new node when pods are respawned.


Updating k8s from 1.24.0 to 1.27.0
1. First find the release of the master/slave nodes
2. cat /etc/*release*
3. Upgrade kubeadm tool as per release version
4. Apply kubeadm upgrade apply v1.27.0
5. Drain the controlplane 
6. Now upgrade kubelet and kubectl
7. reload daemon, restart kubelet
8. k uncordon the master node

#Worker node
1. SSH to worker node, the release of worker node should be same as master node
2. cat /etc/*release*
3. Upgrade kubeadm tool as per release version
4. Apply kubeadm upgrade apply v1.27.0
5. Drain the controlplane 
6. Now upgrade kubelet and kubectl
7. reload daemon, restart kubelet
8. k uncordon the worker node from master node

apt update
apt-cache madison kubeadm
pick the latest to 1.25.0 first
 upgrade master controlplane then go to worker nodes
>apt-mark unhold kubeadm && apt-get update && apt-get install -y kubeadm-1.27.0-00 && apt-mark hold kubeadm
>apt-mark unhold kubeadm && apt-get update && apt install -y kubeadm=1.27.0-00 && apt-mark hold kubeadm

> kubeadm version
> kubeadm upgrade plan
> kubeadm upgrade apply v1.25.10
> kubeadm upgrade apply v1.27.0

kubelet has to be manually upgraded on master nodes--
> k drain controlplane --ignore-daemonsets
> apt-mark unhold kubelet kubectl && apt-get update && apt-get install -y kubelet=1.25.10-00 kubectl-1.25.10-00 && apt-mark hold kubelet kubectl
> apt-mark unhold kubelet kubectl && apt-get update && apt install -y kubelet=1.27.0-00 kubectl=1.27.0-00 && apt-mark hold kubelet kubectl

> sudo systemctl daemon-reload
> sudo systemctl restart kubelet
> k uncordon controlplane


## Worker nodes
> ssh node01
>apt-mark unhold kubeadm && apt-get update && apt-get install -y kubeadm-1.25.10-00 && apt-mark hold kubeadm
>apt-mark unhold kubeadm && apt-get update && apt install -y kubeadm=1.27.0-00 && apt-mark hold kubeadm
>sudo kubeadm upgrade node

kubelet has to be manually upgraded on master nodes--
> k drain node01 --ignore-daemonsets
> apt-mark unhold kubelet kubectl && apt-get update && apt-get install -y kubelet=1.27.0-00 kubectl=1.27.0-00 && apt-mark hold kubelet kubectl
> apt-mark unhold kubelet kubectl && apt-get update && apt install -y kubelet=1.27.0-00 kubectl=1.27.0-00 && apt-mark hold kubelet kubectl

> sudo systemctl daemon-reload
> sudo systemctl restart kubelet
> k uncordon node01     #TO BE DONE ON MASTER NODE

Backup and Restore Methodologies --------
1. Resource Config
   1. backing up resource configs should be on git
   2. k get all --all-namespaces -o yaml > all-deployment-services.yaml
   3. tools like Ark/Velero
2. ETCD cluster
   1. etcd is hosted on master nodes, the back up of data dir while etcd service is created is vital -- data-dir=/var/lib/etcd
   2. ETCDCTL_API=3 etcdctl snapshot save snapshot.db
   ls ./
   ETCDCTL_API=3 etcdctl snapshot status snapshot.db
   #####RESTORE
   3. service kube-apiserver stop
   4. ETCDCTL_API=3 etcdctl snapshot restore snapshot.db --data-dir /var/lib/etcd-from-backup
   5. Use the new data dir while etcd service is created --data-dir /var/lib/etcd-from-backup
   6. systemctl daemon-reload
   7. service etcd restart
   8. service kube-apiserver start
   9. ETCDCTL_API=3 etcdctl snapshot save snapshot.db \
   --ednpoints=https://127.0.0.1:2379 \
   --cacert=/etc/etcd/ca.crt \
   --cert=/etc/etcd/etcd-server.crt \
   --key=/etc/etcd/etcd-server.key 
Note: In managed k8s solution, there is no way to access etcd server, for this case its best to backup by quering the KubeAPIServer
3. Persistent Volumes



For example, if you want to take a snapshot of etcd, use:
etcdctl snapshot save -h and keep a note of the mandatory global options.
Since our ETCD database is TLS-Enabled, the following options are mandatory:
--cacert                                                verify certificates of TLS-enabled secure servers using this CA bundle
--cert                                                    identify secure client using this TLS certificate file
--endpoints=[127.0.0.1:2379]          This is the default as ETCD is running on master node and exposed on localhost 2379.
--key                                                      identify secure client using this TLS key file

Similarly use the help option for snapshot restore to see all available options for restoring the backup.
etcdctl snapshot restore -h

ETCD Server cert /etc/kubernetes/pki/etcd/server.crt
ETCD Server CA cert /etc/kubernetes/pki/etcd/ca.crt

# Save snapshot
ETCDCTL_API=3 etcdctl snapshot save /opt/snapshot-pre-boot.db --endpoints=https://127.0.0.1:2379  --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key

ETCDCTL_API=3 etcdctl snapshot restore /var/lib/etcd-from-backup /opt/snapshot-pre-boot.db --endpoints=https://127.0.0.1:2379  --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key

# Restore snapshot
ETCDCTL_API=3 etcdctl snapshot restore --data-dir /var/lib/etcd-from-backup /opt/snapshot-pre-boot.db --endpoints=https://127.0.0.1:2379  --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key

The hostPath has to be updated
  volumes:
  - hostPath:
      path: /etc/kubernetes/pki/etcd
      type: DirectoryOrCreate
    name: etcd-certs
  - hostPath:  ####### This below path is to be changed in /etc/kubernetes/manifests/etcd.yaml 
      path: /var/lib/etcd
      type: DirectoryOrCreate
    name: etcd-data
/etc/kubernetes/manifests/etcd.yaml 

k config view

kubectl config use-context cluster1
kubectl config use-context cluster2

Find external/stacked etcd in cluster 1 and cluseter2
k describe pod kube-apiserver-cluster1-controlplane -n kube-system
you find the etcd server and get to know if its stacked etcd or not
Command:
kube-apiserver
--etcd-servers=https://127.0.0.1:2379

cluster2 -------
    Command:
      kube-apiserver
      --advertise-address=192.12.101.6
      --allow-privileged=true
      --authorization-mode=Node,RBAC
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --enable-admission-plugins=NodeRestriction
      --enable-bootstrap-token-auth=true
      --etcd-cafile=/etc/kubernetes/pki/etcd/ca.pem
      --etcd-certfile=/etc/kubernetes/pki/etcd/etcd.pem
      --etcd-keyfile=/etc/kubernetes/pki/etcd/etcd-key.pem
      --etcd-servers=https://192.12.101.18:2379



--etcd-servers=https://192.12.101.18



ETCDCTL_API=3 etcdctl snapshot save /opt/cluster1.db --endpoints=https://127.0.0.1:2379  --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key

ETCDCTL_API=3 etcdctl snapshot save /opt/cluster1.db --endpoints=https://192.12.101.3:2379  --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key

ETCDCTL_API=3 etcdctl member list --endpoints=https://192.12.101.3:2379  --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key

ETCDCTL_API=3 etcdctl member list --endpoints=https://127.0.0.1:2379 --cacert=/etc/etcd/pki/ca.pem --cert=/etc/etcd/pki/etcd.pem --key=/etc/etcd/pki/etcd-key.pem
f869812ad900e4e7, started, etcd-server, https://192.13.54.24:2380, https://192.13.54.24:2379, false

k describe pod etcd-cluster1-controlplane -n kube-system | grep server
k describe pod etcd-cluster1-controlplane -n kube-system | grep -i data
# Restore snapshot
ETCDCTL_API=3 etcdctl snapshot restore --data-dir /var/lib/etcd-data-new /opt/cluster2.db --endpoints=https://127.0.0.1:2379  --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key

A ETCD restore doesnt require connection with cacert/cert/keys
ETCDCTL_API=3 etcdctl snapshot restore --data-dir /var/lib/etcd-data-new /opt/cluster2.db   #This is enough

vim /etc/systemd/system/etcd.service
set the data-dir to new folder

etcd-server /opt ➜  systemctl daemon reload
Unknown operation daemon.

etcd-server /opt ✖ systemctl daemon-reload

etcd-server /opt ➜  systemctl restart etcd

etcd-server /opt ➜  systemctl status etcd

k delete pod kube-controller-manager-cluster2-controlplane kube-scheduler-cluster2-controlplane -n kube-system

ssh@cluster2-controlplane
cluster2-controlplane ~ ➜  systemctl restart kubelet
cluster2-controlplane ~ ➜  systemctl status kubelet

In the exam, you won't know if what you did is correct or not as in the practice tests in this course. You must verify your work yourself. For example, if the question is to create a pod with a specific image, you must run the the kubectl describe pod command to verify the pod is created with the correct name and correct image.

https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/#backing-up-an-etcd-cluster
https://github.com/etcd-io/website/blob/main/content/en/docs/v3.5/op-guide/recovery.md
https://www.youtube.com/watch?v=qRPNuT080Hk

CLUSTER MAINTENANCE END
------------------------------------


App Lifecycle Management
--------------------
k rollout status deployment/myapp-deployment
k rollout history deployment/myapp-deployment
k apply –f deployment-definition.yml
k set image deployment/myapp-deployment\nginx=nginx:1.9.1

k get rs
k rollout undo deployment/myapp-deployment
k run nginx --image=nginx


> kubectl create –f deployment-definition.yml
> kubectl get deployments
> kubectl apply –f deployment-definition.yml
> kubectl set image deployment/myapp-deployment nginx=nginx:1.9.1 
> kubectl rollout status deployment/myapp-deployment
> kubectl rollout history deployment/myapp-deployment
> kubectl rollout undo deployment/myapp-deployment

k set image deploy frontend simple-webapp=kodekloud/webapp-color:v2
k set image deploy frontend simple-webapp=kodekloud/webapp-color:v3

controlplane ~ ➜  cat curl-test.sh 
for i in {1..35}; do
   kubectl exec --namespace=kube-public curl -- sh -c 'test=`wget -qO- -T 2  http://webapp-service.default.svc.cluster.local:8080/info 2>&1` && echo "$test OK" || echo "Failed"';
   echo ""
done

controlplane ~ ➜  cat curl-pod.yaml 
apiVersion: v1 
kind: Pod
metadata:
  name: curl
  namespace: kube-public 
spec:
  containers:
  - image: byrnedo/alpine-curl:latest
    name: alpine-curl
    command: ["sleep", "5000"]


Commands Docker and k8s ------
--------------------

FROM Ubuntu
ENTRYPOINT ["sleep"]
CMD ["5"]
docker run ubuntu-sleeper 10
docker run --entrypoint sleep2.0 ubuntu-sleeper 10
docker run --name ubuntu-sleeper ubuntu-sleeper
docker run --name ubuntu-sleeper ubuntu-sleeper 10

apiVersion: v1
kind: Pod
metadata:
  name: ubuntu-sleeper-pod
spec:
  containers:
    - name: ubuntu-sleeper
      image: ubuntu-sleeper
      args: ["10"]

apiVersion: v1
kind: Pod
metadata:
  name: ubuntu-sleeper-pod
spec:
  containers:
    - name: ubuntu-sleeper
      image: ubuntu-sleeper
      command: ["sleep2.0"] #is same as docker ENTRYPOINT
      args: ["10"]          # is same as docker ENTRYPOINT

k create -f pod-fed.yaml

apiVersion: v1 
kind: Pod 
metadata:
  name: ubuntu-sleeper-2 
spec:
  containers:
  - name: ubuntu
    image: ubuntu
    command:
      - "sleep"
      - "5000"

FROM python:3.6-alpine
RUN pip install flask
COPY . /opt/
EXPOSE 8080
WORKDIR /opt
ENTRYPOINT ["python", "app.py"]
CMD ["--color", "red"]

controlplane ~ ➜  cat webapp-color-3/Dockerfile 
FROM python:3.6-alpine
RUN pip install flask
COPY . /opt/
EXPOSE 8080
WORKDIR /opt
ENTRYPOINT ["python", "app.py"]
CMD ["--color", "red"]

controlplane ~ ➜  cat webapp-color-3/webapp-color-pod-2.yaml 
apiVersion: v1 
kind: Pod 
metadata:
  name: webapp-green
  labels:
      name: webapp-green 
spec:
  containers:
  - name: simple-webapp
    image: kodekloud/webapp-color
    command: ["python", "app.py"]
    args: ["--color", "pink"]

k run webapp-green --image=kodekloud/webapp-color -- --color green
k run webapp-green --image=kodekloud/webapp-color --command -- python app2.py -- --color green
k replace --force -f /tmp/k-edit-file.yaml


ENV VARS
--------------------
1. Plain key value
2. ConfigMap
3. Secrets

docker run -e APP_COLOR=pink simple-webapp-color

apiVersion: v1 
kind: Pod 
metadata:
  name: simple-webapp-color
spec:
  containers:
  - name: simple-webapp-color
    image: simple-webapp-color
    ports:
      - containerPort: 8080
    env:
      - name: APP_COLOR
        value: pink

#Configmaps
    env:
      - name: APP_COLOR
        valueFrom:
            configMapKeyRef: 

#Secrets
    env:
      - name: APP_COLOR
        valueFrom:
            secretKeyRef: 

apiVersion: v1 
kind: Pod 
metadata:
  name: simple-webapp-color
spec:
  containers:
  - name: simple-webapp-color
    image: simple-webapp-color
    ports:
      - containerPort: 8080
    envFrom:
      - configMapKeyRef:
          name: app-config
          key: APP_COLOR

apiVersion: v1 
kind: Pod 
metadata:
  name: webapp-color
  labels:
    name: webapp-color
spec:
  containers:
  - name: webapp-color
    image: kodekloud/webapp-color
    envFrom:
      - configMapRef:
          name: webapp-config-map


2 Phases in creating config map
1. Create ConfigMap
2. Inject the configMap into Pod

1. k create configmap    Imperative
2. k create -f           Declarative

k create configmap \
  app-config --from-literal=APP_COLOR=blue

k create configmap \
  app-config2 --from-literal=APP_COLOR=darkblue \ 
  -from-literal=APP_OTHER=disregard

k create configmap <config-name> --from-file=<path-to-file>
k create configmap \
  app-config --from-file=app_config.properties

configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
data:
  APP_COLOR: blue
  APP_MODE: prod

k create -f configmap.yaml


app-config:
APP_COLOR: blue
APP_MODE: prod

mysql-config:
port: 3306
max_allowed_packet: 128M

redis-config:
port: 6379
rdb-compression: yes

k get configmaps
k describe configmaps

apiVersion: v1 
kind: Pod 
metadata:
  name: simple-webapp-color
spec:
  containers:
  - name: simple-webapp-color
    image: simple-webapp-color
    ports:
      - containerPort: 8080
    envFrom:
      - configMapKeyRef:
          name: app-config

spec:
  containers:
    - name: test-container
      image: gcr.io/google_containers/busybox
      command: ["/bin/sh", "-c", "env"]
      env:
        - name: SPECIAL_LEVEL_KEY
          valueFrom:
            configMapKeyRef:
              name: a-config
              key: akey

spec:
  containers:
    - name: test-container
      image: registry.k8s.io/busybox
      command: [ "/bin/sh", "-c", "env" ]
      envFrom:
      - configMapRef:
          name: special-config

spec:
  containers:
    - name: test-container
      image: registry.k8s.io/busybox
      command: [ "/bin/sh", "-c", "env" ]
      env:
        - name: SPECIAL_LEVEL_KEY
          valueFrom:
            configMapKeyRef:
              name: special-config
              key: special.how
        - name: LOG_LEVEL
          valueFrom:
            configMapKeyRef:
              name: env-config
              key: log_level


k get pods -A --no-headers | wc -l 

https://kubernetes.io/docs/concepts/configuration/configmap/
https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/


Secrets --------------------
Secrets are used to store password or keys(sensitive information). They are stored in encoded base64 format.
1. Create Secret
2. Inject Secret

Imperative --------
k create secret generic
k create secret generic <secret-name> --form-literal=<key>=<value>
k create secret generic \
app-secret --form-literal=DB_Host=MYSQL \
           --form-literal=DB_User=root \
           --form-literal=DB_Password=paswrd
k create secret generic <secret-name> --from-file=<path-to-file>
k create secret generic \
app-secret --from-file=app-secret.properties

Declarative --------
k create -f secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: app-secret
data:
  DB_Host: bXlzcWw=
  DB_User: cm9vdA==
  DB_Password: cGFzd3Jk

echo -n 'mysql' | base64
bXlzcWw=
echo -n 'root' | base64
cm9vdA==
echo -n 'paswrd' | base64
cGFzd3Jk

k get secrets
k desscribe secrets
k get secrets app-secret -oyaml

echo -n 'bXlzcWw=' | base64 --decode
mysql
echo -n 'cm9vdA==' | base64 --decode
root
echo -n 'cGFzd3Jk' | base64 --decode
paswrd

apiVersion: v1
kind: Pod
metadata:
  name: nginx
  namespace: dev
  labels:
    app: nginx
spec:
  containers:
    - name: nginx-container
      image: nginx
      ports:
        - containerPort: 8080
      envFrom:
        - secretRef:
            name: app-secret

k create -f pod-def.yaml

SingleEnv ------
env:
  - name: DB_Password
    valueFrom:
      secretKeyRef:
        name: app-secret
        key: DB_Password

Volume -------
volumes:
- name: app-secret-volume
  secret:
    secretName: app-secret

If you check inside volumes inside container you can find secrets at this location

ls /opt/app-secret-volumes
DB_Host         DB_Password         DB_User

cat /opt/app-secret-volumes/DB_Password
paswrd

-- Secrets are not encrypted, only encoded.
-- Do not checkin secret objects to SCM along with code
-- Secrets are not encrypted in ETCD, consider enablling encryption at REST.
https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/

-- Anyone able to create pods/deployments in the same namespace can access the secrets.
-- Consider least privilege access to Secrets - RBAC.
-- Consider 3rd party secret store providers like AWS Provider, Azure Provider, GCP Provider, Vault Provider -- this way the secrets are not stored in etcd but external secret provider those providers take care of most of the security.

https://kubernetes.io/docs/concepts/configuration/secret/#protections
https://kubernetes.io/docs/concepts/configuration/secret/#risks

https://www.vaultproject.io/
Having said that, there are other better ways of handling sensitive data like passwords in Kubernetes, such as using tools like Helm Secrets, HashiCorp Vault. I hope to make a lecture on these in the future.


kubeadm init --apiserver-advertise-address $(hostname -i) --pod-network-cidr 10.5.0.0/16
kubectl apply -f https://raw.githubusercontent.com/cloudnativelabs/kube-router/master/daemonset/kubeadm-kuberouter.yaml
kubectl apply -f https://raw.githubusercontent.com/kubernetes/website/master/content/en/examples/application/nginx-app.yaml
alias k=kubectl
k taint node node1 node-role.kubernetes.io/control-plane:NoSchedule-
yum install wget -y
wget https://go.dev/dl/go1.21.0.linux-amd64.tar.gz
tar -C /usr/local -xzf go1.21.0.linux-amd64.tar.gz
export PATH=$PATH:/usr/local/go/bin                                   
go version
go install github.com/coreos/etcd/etcdctl@latest
cd etcd/bin


ETCDCTL_API=3 ./etcdctl --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key get /registry/secrets/default/

[node1 bin]$ ETCDCTL_API=3 ./etcdctl --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key get /registry/secrets/default/my-sec

/etc/kubernetes/manifests/kube-apiserver.yaml


k taint node node1 node-role.kubernetes.io/control-plane:NoSchedule-  #Remove node taint if present on a node

k create secret generic my-sec --from-literal=key1=secretkeylol

encrypt.yaml
apiVersion: apiserver.config.k8s.io/v1
kind: EncryptionConfiguration
resources:
  - resources:
      - secrets
    providers:
      - aescbc:
          keys:
            - name: key1
              # See the following text for more details about the secret value
              secret: aBRN2uGzTsrN3XSeXgXM5OImPNMKgg9DKfSYzzxzADA=
      - identity: {}
[node1 bin]$ mkdir /etc/kubernetes/enc
[node1 bin]$ mv encrypt.yaml /etc/kubernetes/enc                                     
[node1 bin]$ ls /etc/kubernetes/enc
encrypt.yaml

[node1 bin]$ vi /etc/kubernetes/manifests/kube-apiserver.yaml

spec:
  containers:
  - command:
    - kube-apiserver
    - --advertise-address=192.168.0.8
    ...
    - --encryption-provider-config=/etc/kubernetes/enc/encrypt.yaml
    volumeMounts:             # MOUNTING OF POD VOLUME
    ...
    - name: enc                           # add this line
      mountPath: /etc/kubernetes/enc      # add this line
      readonly: true
  volumes:                    # MOUNTING OF LOCAL CLUSTER VOLUME
  ...
  - name: enc                             # add this line
    hostPath:                             # add this line
      path: /etc/kubernetes/enc           # add this line
      type: DirectoryOrCreate

kube-apiserver will restart
k create secret generic my-sec2 --from-literal=key1=secretkeylol

Now new secret keys will be encrypted
[node1 bin]$ ETCDCTL_API=3 ./etcdctl --cacert=/etc/kubernetes/pki/etcd/ca.crt --cer
etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key get /r
stry/secrets/default/my-sec2 | hexdump -C

Ensure older secrets are also encrypted after above secret encryption steps
kubectl get secrets --all-namespaces -o json | kubectl replace -f -
https://chromium.googlesource.com/external/github.com/coreos/etcd/+/release-3.0/etcdctl/README.md

MULTI CONTAINER Pod --------------------
apiVersion: v1
kind: Pod
metadata:
  name: nginx
  namespace: dev
  labels:
    app: nginx
spec:
  containers:
    - name: nginx-container
      image: nginx
      ports:
        - containerPort: 8080
    - name: log-agent
      image: log-agent
  

k run yellow --image=busybox --dry-run=client -o yaml
k -n elastic-stack exec -it app --  /log/app.log

apiVersion: v1
kind: Pod
metadata:
  labels:
    name: app
  name: app
  namespace: elastic-stack
spec:
  containers:
  - image: kodekloud/event-simulator
    name: app
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /log
      name: log-volume
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: kube-api-access-h68qm
      readOnly: true
  - image: kodekloud/filebeat-configured
    name: sidecar
    volumeMounts:
    - mountPath: /var/log/event-simulator/ #LOGS ARE READ AND SENT TO ELASTICSEARCG
      name: log-volume

MULTI CONTAINER PODS DESIGN PATTERN
1. Sidecar
2. Adapter
3. Ambassador


INIT CONTAINER --------------------
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
spec:
  containers:
  - name: myapp-container
    image: busybox:1.28
    command: ['sh', '-c', 'echo The app is running! && sleep 3600']
  initContainers:
  - name: init-myservice
    image: busybox:1.28
    command: ['sh', '-c', 'until nslookup myservice; do echo waiting for myservice; sleep 2; done;']
  - name: init-mydb
    image: busybox:1.28
    command: ['sh', '-c', 'until nslookup mydb; do echo waiting for mydb; sleep 2; done;']

https://kubernetes.io/docs/concepts/workloads/pods/init-containers/

SELF HEALING APP
Kubernetes supports self-healing applications through ReplicaSets and Replication Controllers. The replication controller helps in ensuring that a POD is re-created automatically when the application within the POD crashes. It helps in ensuring enough replicas of the application are running at all times.
Kubernetes provides additional support to check the health of applications running within PODs and take necessary actions through Liveness and Readiness Probes. However these are not required for the CKA exam and as such they are not covered here. These are topics for the Certified Kubernetes Application Developers (CKAD) exam and are covered in the CKAD course.

Logging and Monitor
--------------------
https://kubernetes.io/docs/tasks/debug/debug-cluster/resource-usage-monitoring/

https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/

kubelet is responsible for receiving instructions from the k8s API master server and runnng pods on the nodes.
Kubelet also contains a sub component known as CAdvisor/Container Advisor.
Cadvisor is responsible for retrieving performance metrics from pods and exposing them through the kubelet API to make the metrics available for the Metrics Server. 
> minicube addons enable metrics-server
> git clone https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/
> k create -f deploy/1.27+/
> k top node
> k top pod

> git clone https://github.com/kodekloudhub/kubernetes-metrics-server.git

k create -f event-sim.yaml
apiVersion: v1
kind: Pod
metadata:
  name: event-simulator-pod
  labels:
spec:
  containers:
    - name: event-simulator
      image: kodekloud/event-simulator

k logs -f event-simulator-pod







Scheduler
------
https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/

apiVersion: v1
kind: Pod
metadata:
  name: nginx
  namespace: dev
  labels:
    app: nginx
spec:
  containers:
    - name: nginx-container
      image: nginx
      ports:
        - containerPort: 8080
  nodeName: node02


For already existing pod, if you need to assign the pod to a node, then you will need to apply below binding object yaml to a pod. Send a POST request to the pods binding data.

apiVersion: v1
kind: Binding
metadata:
  name: nginx
target:
  apiVersion: v1
  kind: Node
  name: node2

curl --header "Content-Type:application/json" --request POST --data 'above yaml file' http://$$SERVER/api/v1/namespaces/default/pods/$PODNAME/binding/

k describe pod nginx - the node is empty
k get pods -n kube-system - you should see the scheduler pod
k replace --force -f nginx.yaml
k get pods -owide

LABELS -------
k get pods -l env=prod,bu=finance --no-headers
k get pods -l env=prod,bu=finance --no-headers | wc -l
k get all -l env=prod,bu=finance --no-headers | wc -l

TAINTS AND TOLERATIONS -------

k taint nodes node-name key=value:taint-effect
No schedule | PreferNoSchedule | NoExecute - evicted

NoSchedule - Pods will not be scheduled on the nodes
PreferNoSchedule - system will try to avoid placing the pod, but not guaranteed
NoExecute - new pods will not be scheduled and existing ones are evicted 

k taint nodes node1 app=blue:NoSchedule
k taint nodes node1 app=blue:NoSchedule-
k describe node kubemaster | grep Taint
k describe node controlplane | grep -i Taint

k taint nodes node01 spray=mortein:NoSchedule
k taint nodes controlplane node-role.kubernetes.io/control-plane:NoSchedule-

k run mosquito --image=nginx --port=8080

apiVersion: v1
kind: Pod
metadata:
  labels:
    name: bee
  name: bee
spec:
  containers:
    - name: nginx-container
      image: 
  restartPolicy: Always
  dnsPolicy: ClusterFirst
  tolerations:
  - key: "spray"
    operator: "Equal"
    value: "mortein"
    effect: NoSchedule

k run bee --image=nginx --dry-run=client -o yaml > bee.yaml


Node selector
k label nodes <node-name> <key>=<value>
k label nodes node-01 size=Large

apiVersion: v1
kind: Pod
metadata:
  labels:
    name: bee
  name: bee
spec:
  containers:
    - name: nginx-container
      image: nginx
  nodeSelector:
    size: Large

apiVersion: v1
kind: Pod
metadata:
  labels:
    name: bee
  name: bee
spec:
  containers:
    - name: nginx-container
      image: nginx
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: size
            operator: In
            values:
            - Large
            - Medium

operator: In, NotIn, Exists

NodeAffinityTypes
available - 
requiredDuringSchedulingIgnoredDuringExecution:
preferredDuringSchedulingIgnoredDuringExecution:

planned - 
requiredDuringSchedulingRequiredDuringExecution:


apiVersion: apps/v1
kind: Deployment
metadata:
  name: blue
  labels:
    color: blue
spec:
  replicas: 3
  template:
    metadata:
      name: blue
      labels:
        color: blue
    spec:
      containers:
        - name: nginx-container
          image: nginx
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: color
                operator: In
                values:
                - blue
  selector:
    matchLabels:
      color: blue

# k create deployment red --image=nginx --replicas=2 --dry-run=client -oyaml > red2.yaml
# The Deployment "red" is invalid: spec.template.spec.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.
# nodeSelectorTerms[0].matchExpressions[0].values: Forbidden: may not be specified when `operator` is 'Exists' or 'DoesNotExist'

---
apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: red
  name: red
spec:
  replicas: 2
  selector:
    matchLabels:
      app: red
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: red
    spec:
      containers:
      - image: nginx
        name: nginx
        resources: {}
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: node-role.kubernetes.io/control-plane
                operator: Exists
status: {}

Resource Requests --------

apiVersion: v1
kind: Pod
metadata:
  labels:
    name: bee
  name: bee
spec:
  containers:
  - name: nginx-container
    image: nginx
    resources:
      requests:
        memory: "2Gi"
        cpu: 2
      limits:
        memory: "2Gi"
        cpu: 2


CPU ----
0.1 CPU <-> 100M
1M

1 CPU
1 AWS VCPU
1 GCP Core
1 Azure Core
1 Hyperthrea


MEMORY -----
256Mi = 268435456
268M
1G    Gigabyte
1M
1K
1Gi   Gibibyte
1Mi   Mebibyte
1Ki   Kibibyte
 
CPU ----
NO REQUESTS
NO LIMITS

NO REQUESTS
LIMITS

REQUESTS
LIMITS

This is the right choice in cpu selection
REQUESTS
NO LIMITS

MEMORY ----
NO REQUESTS
NO LIMITS

NO REQUESTS
LIMITS

REQUESTS
LIMITS

REQUESTS
NO LIMITS


LimitRange -----
CPU---
apiVersion: v1
kind: LimitRange
metadata:
  name: cpu-resource-constraint
spec:
  limits:
  - default:
      cpu: 500m
    defaultRequest:
      cpu: 500m
    max:
      cpu: "1"
    min:
      cpu: 100m
    type: Container

MEMORY---
apiVersion: v1
kind: LimitRange
metadata:
  name: memory-resource-constraint
spec:
  limits:
  - default:
      memory: 1Gi
    defaultRequest:
      memory: 1Gi
    max:
      memory: 1Gi
    min:
      memory: 500Mi
    type: Container

RESORCE QUOTA ---
created at namespace level

apiVersion: v1
kind: ResourceQuota
metadata:
  name: my-resource-quota
spec:
  hard:
    requests.cpu: 4
    requests.memory: 4Gi
    limits.cpu: 10
    limits.memory: 10Gi

which will give the complete spec with node name, or:
kubectl get nodes -o json | jq '.items[].spec'
kubectl get nodes -o json | jq '.items[].spec.taints'


DAEMON SETS ---
A pod in every node, this is used in logging and monitoring events in a cluster.
kube-proxy can be deployed as a daemonset in cluster.
weave-net deploys a networking pod in each cluster.

apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: monitoring-daemon
  labels: 
    app: monitoring-agent
spec:
  selector:
    matchLabels:
      app: monitoring-agent
  template:
    metadata:
      labels:
        app: monitoring-agent
    spec:
      containers:
        - name: monitoring-agent
          image: monitoring-agent

DS uses default scheduler and node affinity rules to schedule pods on nodes





Static pod Busybox
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: static-busybox
  name: static-busybox
  namespace: default
spec:
  containers:
  - image: busybox
    imagePullPolicy: Always
    name: busybox
    command: ["/bin/sh", "-ec", "sleep 1000"]

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: static-busybox
  name: static-busybox
spec:
  containers:
  - command:
    - sleep
    - "1000"
    image: busybox
    name: static-busybox
  restartPolicy: Never

Multiple Scheduler ------

apiVersion: kubescheduler.config.k8s.io/v1
kind: KubeSchedulerConfiguration
profiles:
- schedulerName: my-scheduler
leaderElection:
  leaderElect: true
  resourceNamespace: kube-system
  resourceName: lock-object-my-scheduler

my-custom-scheduler.yaml
apiVersion: v1
kind: Pod
metadata:
  name: my-custom-scheduler
  namespace: kube-system
spec:
  containers:
  - image: busybox
    name: busybox
    command:
    - kube-scheduler
    - --address=127.0.0.1
    - --kubeconfig=/etc/kubernetes/scheduler.conf
    - --config=/etc/kubernetes/my-scheduler-config.yaml
k get events -owide
k logs custom-scheduler -n=kube-system

k create configmap my-scheduler-config --from-file=/root/my-scheduler-config.yaml -n kube-system


controlplane ~ ➜  cat my-scheduler-config.yaml 
apiVersion: kubescheduler.config.k8s.io/v1beta2
kind: KubeSchedulerConfiguration
profiles:
  - schedulerName: my-scheduler
leaderElection:
  leaderElect: false

controlplane ~ ➜  cat nginx-pod.yaml 
apiVersion: v1 
kind: Pod 
metadata:
  name: nginx 
spec:
  schedulerName: my-scheduler
  containers:
  - image: nginx
    name: nginx

controlplane ~ ➜  cat my-scheduler-configmap.yaml
apiVersion: v1
data:
  my-scheduler-config.yaml: |
    apiVersion: kubescheduler.config.k8s.io/v1beta2
    kind: KubeSchedulerConfiguration
    profiles:
      - schedulerName: my-scheduler
    leaderElection:
      leaderElect: false
kind: ConfigMap
metadata:
  creationTimestamp: null
  name: my-scheduler-config
  namespace: kube-system

controlplane ~ ➜  cat my-scheduler.yaml 
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: my-scheduler
  name: my-scheduler
  namespace: kube-system
spec:
  serviceAccountName: my-scheduler
  containers:
  - command:
    - /usr/local/bin/kube-scheduler
    - --config=/etc/kubernetes/my-scheduler/my-scheduler-config.yaml
    image: registry.k8s.io/kube-scheduler:v1.27.0
    livenessProbe:
      httpGet:
        path: /healthz
        port: 10259
        scheme: HTTPS
      initialDelaySeconds: 15
    name: kube-second-scheduler
    readinessProbe:
      httpGet:
        path: /healthz
        port: 10259
        scheme: HTTPS
    resources:
      requests:
        cpu: '0.1'
    securityContext:
      privileged: false
    volumeMounts:
      - name: config-volume
        mountPath: /etc/kubernetes/my-scheduler
  hostNetwork: false
  hostPID: false
  volumes:
    - name: config-volume
      configMap:
        name: my-scheduler-config


Scheduler Profiles -----
Scheduling Queue -> Filtering -> Scoring -> Binding
Scheduling Queue(PrioritySort) -> Filtering(NodeResourcesFit,NodeName,NodeUnschedulable) -> Scoring(NodeResourcesFit, ImageLocality) -> Binding(DefaultBinder)
- Extension Point can be plugged to the scheduling plugins
- Scheduling Queue(queue sort) -> Filtering(preFilter,filter,postFilter) -> Scoring(preScore,score,reserve,permit) -> Binding(preBind,bind,postBind)
apiVersion: v1 
kind: Pod 
metadata:
  name: nginx 
spec:
  priorityClassName: high-priority
  containers:
  - image: nginx
    name: nginx

Sample Multiple Scheduler Profiles -----
apiVersion: kubescheduler.config.k8s.io/v1
kind: KubeSchedulerConfiguration
profiles:
- schedulerName: my-scheduler-2
  plugins:
    score:
      disabled:
        - name: TaintToleration
      enabled:
        - name: CustomPluginA
        - name: CustomPluginB
- schedulerName: my-scheduler-3
  plugins:
    preScore:
      disabled:
        - name: '*'
    score:
      disabled:
        - name: '*'
- schedulerName: my-scheduler-4

https://kubernetes.io/docs/tasks/extend-kubernetes/configure-multiple-schedulers/

Scheduler code hierarchy overview
https://github.com/kubernetes/community/blob/master/contributors/devel/sig-scheduling/scheduling_code_hierarchy_overview.md

Advance Scheduling in K8s ------
https://kubernetes.io/blog/2017/03/advanced-scheduling-in-kubernetes/

How does Kubernetes' scheduler work?
https://jvns.ca/blog/2017/07/27/how-does-the-kubernetes-scheduler-work/
https://stackoverflow.com/questions/28857993/how-does-kubernetes-scheduler-work