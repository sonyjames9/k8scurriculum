Scheduler
------
https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/

apiVersion: v1
kind: Pod
metadata:
  name: nginx
  namespace: dev
  labels:
    app: nginx
spec:
  containers:
    - name: nginx-container
      image: nginx
      ports:
        - containerPort: 8080
  nodeName: node02


For already existing pod, if you need to assign the pod to a node, then you will need to apply below binding object yaml to a pod. Send a POST request to the pods binding data.

apiVersion: v1
kind: Binding
metadata:
  name: nginx
target:
  apiVersion: v1
  kind: Node
  name: node2

curl --header "Content-Type:application/json" --request POST --data 'above yaml file' http://$$SERVER/api/v1/namespaces/default/pods/$PODNAME/binding/

k describe pod nginx - the node is empty
k get pods -n kube-system - you should see the scheduler pod
k replace --force -f nginx.yaml
k get pods -owide

LABELS -------
k get pods -l env=prod,bu=finance --no-headers
k get pods -l env=prod,bu=finance --no-headers | wc -l
k get all -l env=prod,bu=finance --no-headers | wc -l

TAINTS AND TOLERATIONS -------

https://blog.kubecost.com/blog/kubernetes-taints/

k taint nodes node-name key=value:taint-effect
No schedule | PreferNoSchedule | NoExecute - evicted

NoSchedule - Pods will not be scheduled on the nodes
PreferNoSchedule - system will try to avoid placing the pod, but not guaranteed
NoExecute - new pods will not be scheduled and existing ones are evicted 

k taint nodes node1 app=blue:NoSchedule
k taint nodes node1 app=blue:NoSchedule-
k describe node kubemaster | grep Taint
k describe node controlplane | grep -i Taint

k taint nodes node01 spray=mortein:NoSchedule
k taint nodes controlplane node-role.kubernetes.io/control-plane:NoSchedule-

k run mosquito --image=nginx --port=8080

apiVersion: v1
kind: Pod
metadata:
  labels:
    name: bee
  name: bee
spec:
  containers:
    - name: nginx-container
      image: 
  restartPolicy: Always
  dnsPolicy: ClusterFirst
  tolerations:
  - key: "spray"
    operator: "Equal"
    value: "mortein"
    effect: NoSchedule

k run bee --image=nginx --dry-run=client -o yaml > bee.yaml


Node selector
k label nodes <node-name> <key>=<value>
k label nodes node-01 size=Large

apiVersion: v1
kind: Pod
metadata:
  labels:
    name: bee
  name: bee
spec:
  containers:
    - name: nginx-container
      image: nginx
  nodeSelector:
    size: Large

apiVersion: v1
kind: Pod
metadata:
  labels:
    name: bee
  name: bee
spec:
  containers:
    - name: nginx-container
      image: nginx
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: size
            operator: In
            values:
            - Large
            - Medium

operator: In, NotIn, Exists

NodeAffinityTypes
available - 
requiredDuringSchedulingIgnoredDuringExecution:
preferredDuringSchedulingIgnoredDuringExecution:

planned - 
requiredDuringSchedulingRequiredDuringExecution:


apiVersion: apps/v1
kind: Deployment
metadata:
  name: blue
  labels:
    color: blue
spec:
  replicas: 3
  template:
    metadata:
      name: blue
      labels:
        color: blue
    spec:
      containers:
        - name: nginx-container
          image: nginx
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: color
                operator: In
                values:
                - blue
  selector:
    matchLabels:
      color: blue

# k create deployment red --image=nginx --replicas=2 --dry-run=client -oyaml > red2.yaml
# The Deployment "red" is invalid: spec.template.spec.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.
# nodeSelectorTerms[0].matchExpressions[0].values: Forbidden: may not be specified when `operator` is 'Exists' or 'DoesNotExist'

---
apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: red
  name: red
spec:
  replicas: 2
  selector:
    matchLabels:
      app: red
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: red
    spec:
      containers:
      - image: nginx
        name: nginx
        resources: {}
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: node-role.kubernetes.io/control-plane
                operator: Exists
status: {}

Resource Requests --------

apiVersion: v1
kind: Pod
metadata:
  labels:
    name: bee
  name: bee
spec:
  containers:
  - name: nginx-container
    image: nginx
    resources:
      requests:
        memory: "2Gi"
        cpu: 2
      limits:
        memory: "2Gi"
        cpu: 2


CPU ----
0.1 CPU <-> 100M
1M

1 CPU
1 AWS VCPU
1 GCP Core
1 Azure Core
1 Hyperthrea


MEMORY -----
256Mi = 268435456
268M
1G    Gigabyte
1M
1K
1Gi   Gibibyte
1Mi   Mebibyte
1Ki   Kibibyte
 
CPU ----
NO REQUESTS
NO LIMITS

NO REQUESTS
LIMITS

REQUESTS
LIMITS

This is the right choice in cpu selection
REQUESTS
NO LIMITS

MEMORY ----
NO REQUESTS
NO LIMITS

NO REQUESTS
LIMITS

REQUESTS
LIMITS

REQUESTS
NO LIMITS


LimitRange -----
CPU---
apiVersion: v1
kind: LimitRange
metadata:
  name: cpu-resource-constraint
spec:
  limits:
  - default:
      cpu: 500m
    defaultRequest:
      cpu: 500m
    max:
      cpu: "1"
    min:
      cpu: 100m
    type: Container

MEMORY---
apiVersion: v1
kind: LimitRange
metadata:
  name: memory-resource-constraint
spec:
  limits:
  - default:
      memory: 1Gi
    defaultRequest:
      memory: 1Gi
    max:
      memory: 1Gi
    min:
      memory: 500Mi
    type: Container

RESORCE QUOTA ---
created at namespace level

apiVersion: v1
kind: ResourceQuota
metadata:
  name: my-resource-quota
spec:
  hard:
    requests.cpu: 4
    requests.memory: 4Gi
    limits.cpu: 10
    limits.memory: 10Gi

which will give the complete spec with node name, or:
kubectl get nodes -o json | jq '.items[].spec'
kubectl get nodes -o json | jq '.items[].spec.taints'


DAEMON SETS ---
A pod in every node, this is used in logging and monitoring events in a cluster.
kube-proxy can be deployed as a daemonset in cluster.
weave-net deploys a networking pod in each cluster.

apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: monitoring-daemon
  labels: 
    app: monitoring-agent
spec:
  selector:
    matchLabels:
      app: monitoring-agent
  template:
    metadata:
      labels:
        app: monitoring-agent
    spec:
      containers:
        - name: monitoring-agent
          image: monitoring-agent

DS uses default scheduler and node affinity rules to schedule pods on nodes





Static pod Busybox
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: static-busybox
  name: static-busybox
  namespace: default
spec:
  containers:
  - image: busybox
    imagePullPolicy: Always
    name: busybox
    command: ["/bin/sh", "-ec", "sleep 1000"]

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: static-busybox
  name: static-busybox
spec:
  containers:
  - command:
    - sleep
    - "1000"
    image: busybox
    name: static-busybox
  restartPolicy: Never

Multiple Scheduler ------

apiVersion: kubescheduler.config.k8s.io/v1
kind: KubeSchedulerConfiguration
profiles:
- schedulerName: my-scheduler
leaderElection:
  leaderElect: true
  resourceNamespace: kube-system
  resourceName: lock-object-my-scheduler

my-custom-scheduler.yaml
apiVersion: v1
kind: Pod
metadata:
  name: my-custom-scheduler
  namespace: kube-system
spec:
  containers:
  - image: busybox
    name: busybox
    command:
    - kube-scheduler
    - --address=127.0.0.1
    - --kubeconfig=/etc/kubernetes/scheduler.conf
    - --config=/etc/kubernetes/my-scheduler-config.yaml
k get events -owide
k logs custom-scheduler -n=kube-system

k create configmap my-scheduler-config --from-file=/root/my-scheduler-config.yaml -n kube-system


controlplane ~ ➜  cat my-scheduler-config.yaml 
apiVersion: kubescheduler.config.k8s.io/v1beta2
kind: KubeSchedulerConfiguration
profiles:
  - schedulerName: my-scheduler
leaderElection:
  leaderElect: false

controlplane ~ ➜  cat nginx-pod.yaml 
apiVersion: v1 
kind: Pod 
metadata:
  name: nginx 
spec:
  schedulerName: my-scheduler
  containers:
  - image: nginx
    name: nginx

controlplane ~ ➜  cat my-scheduler-configmap.yaml
apiVersion: v1
data:
  my-scheduler-config.yaml: |
    apiVersion: kubescheduler.config.k8s.io/v1beta2
    kind: KubeSchedulerConfiguration
    profiles:
      - schedulerName: my-scheduler
    leaderElection:
      leaderElect: false
kind: ConfigMap
metadata:
  creationTimestamp: null
  name: my-scheduler-config
  namespace: kube-system

controlplane ~ ➜  cat my-scheduler.yaml 
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: my-scheduler
  name: my-scheduler
  namespace: kube-system
spec:
  serviceAccountName: my-scheduler
  containers:
  - command:
    - /usr/local/bin/kube-scheduler
    - --config=/etc/kubernetes/my-scheduler/my-scheduler-config.yaml
    image: registry.k8s.io/kube-scheduler:v1.27.0
    livenessProbe:
      httpGet:
        path: /healthz
        port: 10259
        scheme: HTTPS
      initialDelaySeconds: 15
    name: kube-second-scheduler
    readinessProbe:
      httpGet:
        path: /healthz
        port: 10259
        scheme: HTTPS
    resources:
      requests:
        cpu: '0.1'
    securityContext:
      privileged: false
    volumeMounts:
      - name: config-volume
        mountPath: /etc/kubernetes/my-scheduler
  hostNetwork: false
  hostPID: false
  volumes:
    - name: config-volume
      configMap:
        name: my-scheduler-config


Scheduler Profiles -----
Scheduling Queue -> Filtering -> Scoring -> Binding
Scheduling Queue(PrioritySort) -> Filtering(NodeResourcesFit,NodeName,NodeUnschedulable) -> Scoring(NodeResourcesFit, ImageLocality) -> Binding(DefaultBinder)
- Extension Point can be plugged to the scheduling plugins
- Scheduling Queue(queue sort) -> Filtering(preFilter,filter,postFilter) -> Scoring(preScore,score,reserve,permit) -> Binding(preBind,bind,postBind)
apiVersion: v1 
kind: Pod 
metadata:
  name: nginx 
spec:
  priorityClassName: high-priority
  containers:
  - image: nginx
    name: nginx

Sample Multiple Scheduler Profiles -----
apiVersion: kubescheduler.config.k8s.io/v1
kind: KubeSchedulerConfiguration
profiles:
- schedulerName: my-scheduler-2
  plugins:
    score:
      disabled:
        - name: TaintToleration
      enabled:
        - name: CustomPluginA
        - name: CustomPluginB
- schedulerName: my-scheduler-3
  plugins:
    preScore:
      disabled:
        - name: '*'
    score:
      disabled:
        - name: '*'
- schedulerName: my-scheduler-4

https://kubernetes.io/docs/tasks/extend-kubernetes/configure-multiple-schedulers/

Scheduler code hierarchy overview
https://github.com/kubernetes/community/blob/master/contributors/devel/sig-scheduling/scheduling_code_hierarchy_overview.md

Advance Scheduling in K8s ------
https://kubernetes.io/blog/2017/03/advanced-scheduling-in-kubernetes/

How does Kubernetes' scheduler work?
https://jvns.ca/blog/2017/07/27/how-does-the-kubernetes-scheduler-work/
https://stackoverflow.com/questions/28857993/how-does-kubernetes-scheduler-work

----------------------






https://www.cncf.io/certification/cka/

https://github.com/cncf/curriculum

https://www.cncf.io/certification/candidate-handbook

Exam Tips: http://training.linuxfoundation.org/go//Important-Tips-CKA-CKAD

Use the code - KODEKLOUD20 - while registering for the CKA or CKAD exams at Linux Foundation to get a 20% discount.

https://docs.linuxfoundation.org/tc-docs/certification/faq-cka-ckad-cks

https://training.linuxfoundation.org/?s=kubernetes
https://docs.linuxfoundation.org/tc-docs/certification/tips-cka-and-ckad

https://github.com/sonyjames9/k8scurriculum

https://trainingportal.linuxfoundation.org/collections/kubernetes-developer
https://trainingportal.linuxfoundation.org/courses/certified-kubernetes-application-developer-ckad

https://www.cncf.io/certification/ckad/   developer
https://www.cncf.io/certification/cka/    administrator
https://training.linuxfoundation.org/skillcred/helm/

https://training.linuxfoundation.org/training/introduction-to-devops-and-site-reliability-engineering-lfs162/

https://training.play-with-kubernetes.com/kubernetes-workshop/
https://training.play-with-docker.com/alacart/


https://app.slack.com/client/TDSBA9B9V/C018VNH04LB/thread/C02LS58EGQ4-1687051526.410079   CodeKloud
https://app.slack.com/client/T09NY5SBT/C09NXKJKA/thread/C09NXKJKA-1688415137.305689       Kubernetes
https://kodekloud.com/pages/community

https://github.com/kodekloudhub/community-faq
https://github.com/kodekloudhub/community-faq#all-exams

https://opensource.com/article/20/7/tmux-cheat-sheet
https://vim.rtorr.com/
https://github.com/kodekloudhub/community-faq/blob/main/docs/etcd-faq.md


https://github.com/mmumshad/kubernetes-the-hard-way
https://github.com/kodekloudhub/certified-kubernetes-administrator-course 

https://kubernetes.io/docs/reference/kubectl/cheatsheet/

https://helm.sh/docs/intro/

https://kubernetes.io/docs/concepts/overview/kubernetes-api/

https://www.youtube.com/@kubesimplify/featured


NTT is hiring!
We NTT are looking for engineers who work in Open Source communities like containerd, Docker/Moby, Kubernetes, and their relevant projects. Visit https://www.rd.ntt/e/sic/recruit/ to see how to join us.

https://github.com/containerd/nerdctl
https://medium.com/nttlabs/nerdctl-359311b32d0e

https://github.com/containerd/containerd/blob/main/docs/getting-started.md
https://github.com/containerd/nerdctl

https://kubernetes.io/docs/reference/tools/map-crictl-dockercli/

crictl --runtime-endpoint
unix:///var/run/dockershim.sock
unix:///run/contianerd/containerd.sock
unix:///run/crio/crio.sock
unix:///var/run/cri-dockerd.sock

export CONTAINER_RUNTIME_ENDPOINT

ctr     contianerD
nerdctl containerD
crictl  k8s and all cri compatible runtimes


ETCD
-------
ETCDCTL is the CLI tool used to interact with ETCD.

ETCDCTL can interact with ETCD Server using 2 API versions - Version 2 and Version 3.  By default its set to use Version 2. Each version has different sets of commands.

For example ETCDCTL version 2 supports the following commands:

etcdctl backup
etcdctl cluster-health
etcdctl mk
etcdctl mkdir
etcdctl set


Whereas the commands are different in version 3

etcdctl snapshot save 
etcdctl endpoint health
etcdctl get
etcdctl put

To set the right version of API set the environment variable ETCDCTL_API command

export ETCDCTL_API=3



When API version is not set, it is assumed to be set to version 2. And version 3 commands listed above don't work. When API version is set to version 3, version 2 commands listed above don't work.



Apart from that, you must also specify path to certificate files so that ETCDCTL can authenticate to the ETCD API Server. The certificate files are available in the etcd-master at the following path. We discuss more about certificates in the security section of this course. So don't worry if this looks complex:

--cacert /etc/kubernetes/pki/etcd/ca.crt     
--cert /etc/kubernetes/pki/etcd/server.crt     
--key /etc/kubernetes/pki/etcd/server.key


So for the commands I showed in the previous video to work you must specify the ETCDCTL API version and path to certificate files. Below is the final form:



kubectl exec etcd-master -n kube-system -- sh -c "ETCDCTL_API=3 etcdctl get / --prefix --keys-only --limit=10 --cacert /etc/kubernetes/pki/etcd/ca.crt --cert /etc/kubernetes/pki/etcd/server.crt  --key /etc/kubernetes/pki/etcd/server.key" 



Kube API Scheduler

Kube Controller Manager

Kube Scheduler

Kubelet

KubeProxy	
process that runs on each node in k8s cluster, it looks for new services and everytime a svc is created it creates appropriate rules on each node to fwd traffic to those svcs to backend pod.One way is to use ip tables. It creates iptables rules on each node in cluster to forward traffic.


Pods.yaml:

apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
    type: front-end
spec:
  containers:
    - name: nginx-container
      image: nginx 

#Create/Spawn a k8s yaml object
kubectl create -f pod-def.yml
kubectl apply -f pod-def.yml

#Replace a k8s yaml object
kubectl replace -f pod-def.yml

kubectl get pods
kubectl describe pod pod-name

kubernetes labs
https://labs.play-with-k8s.com

alias k=kubectl
kubeadm init --apiserver-advertise-address $(hostname -i) --pod-network-cidr 10.5.0.0/16
kubectl apply -f https://raw.githubusercontent.com/cloudnativelabs/kube-router/master/daemonset/kubeadm-kuberouter.yaml
kubectl apply -f https://raw.githubusercontent.com/kubernetes/website/master/content/en/examples/application/nginx-app.yaml

mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config


Replication Controller -----

apiVersion: v1
kind: ReplicationController
metadata:
  name: myapp-rc
  labels: 
    app: myapp
    type: front-end  
spec:
  replicas: 2    
  template:
    metadata:
      name: myapp-pod
      labels:
        app: myapp
        type: front-end
    spec:
      containers:
        - name: nginx-container
          image: nginx 

Replica Set -----

apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: myapp-rs
  labels: 
    app: myapp
    type: front-end  
spec:
  replicas: 2    
  template:
    metadata:
      name: myapp-pod
      labels:
        app: myapp
        type: front-end
    spec:
      containers:
        - name: nginx-container
          image: nginx
  selector:
    matchLabels:
      type: front-end

# Update an existing RS, but this does not work
kubectl scale --replicas=6 -f rs.yaml

# Update by the replicas and replicaset name
kubectl scale --replicas=6  replicaset myapp-rs

k create -f rs.yml
k get rs
k delete rs myapp-rs
k replace -f rs.yaml


Deployment -----

apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-deployment
  labels: 
    app: myapp
    type: front-end  
spec:
  replicas: 2    
  template:
    metadata:
      name: myapp-pod
      labels:
        app: myapp
        type: front-end
    spec:
      containers:
        - name: nginx-container
          image: nginx
  selector:
    matchLabels:
      type: front-end


Service -----

apiVersion: v1
kind: Service
metadata:
  name: myapp-service
  labels: 
    app: myapp
    type: front-end  
spec:
  type: NodePort
  ports:
    - targetPort: 80
      port: 80
      NodePort: 30008
      selector:
        app: myapp
        type: front-end  
        
- if we dont provide targetPort, then port key value pair is considered as targetPort
- if NodePort is not provided, then a value between 30000 & 32767 is automatically allocated

k create -f svc-def.yaml
k get svc

curl http://ip:NodePort


ClusterIP
apiVersion: v1
kind: Service
metadata:
  name: backend
  labels:
    app: myapp
    type: front-end
spec:
  type: ClusterIP
  ports:
    - targetPort: 80
      port: 80
      selector:
        app: myapp
        type: front-end

LoadBalancer
apiVersion: v1
kind: Service
metadata:
  name: backend
  labels:
    app: myapp
    type: front-end
spec:
  type: LoadBalancer 
  ports:
    - targetPort: 80
      port: 80
      selector:
        app: myapp
        type: front-end


namespaces

db-svc.dev.svc.cluster.local
svc name.ns.svc.domain

db-service.dev.svc.cluster.local

Pods.yaml with ns:

apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  namespace: dev
  labels:
    app: myapp
    type: front-end
spec:
  containers:
    - name: nginx-container
      image: nginx

create ns
apiVersion: v1
kind: Namespace
metadata:
  name: myapp-pod

k create ns dev
k get pods --all-namespaces
k get pods -n default

k config set-context $(k config current-context) --namespace=dev

Compute-Quota.yaml -----
apiVersion: v1
kind: ResourceQuota
metadata:
  name: Compute-Quota.yaml
  namespace: dev
spec:
  hard:
   pods: "10"
   requests.cpu: "4"
   requests.memory: "5Gi"
   limits.cpu: "10"
   limits.memory: "10Gi"


Imperative and Declarative -----

the k8s commands is Imperative
Create Objects ---
k run --image=nginx nginx
k create deployment --image=nginx nginx
k expose deployment nginx --port 80
Update Objects ---
k edit deployment nginx
k scale deployment nginx --replicas=5
k set image deployment nginx nginx=nginx:1.18
k create -f nginx.yaml
k replace -f nginx.yaml
k delete -f nginx.yaml


the yaml definition is declarative, like how you create a pod, deployment, etc
k apply -f nginx.yaml

--dry-run: By default as soon as the command is run, the resource will be created. If you simply want to test your command , use the --dry-run=client option. This will not create the resource, instead, tell you whether the resource can be created and if your command is right.
-o yaml: This will output the resource definition in YAML format on screen.

Use the above two in combination to generate a resource definition file quickly, that you can then modify and create resources as required, instead of creating the files from scratch.

Another way to do this is to save the YAML definition to a file and modify

kubectl create deployment nginx --image=nginx --dry-run=client -o yaml > nginx-deployment.yaml
k create deployment webapp --image=kodekloud/webapp-color --replicas=3 --dry-run=client -o yaml > webapp-deployment.yaml
k create deployment webapp --image=kodekloud/webapp-color --replicas=3
k create deployment redis-deploy --image=redis --replicas=2 -n dev-ns

You can then update the YAML file with the replicas or any other field before creating the deployment.

Service
Create a Service named redis-service of type ClusterIP to expose pod redis on port 6379

kubectl expose pod redis --port=6379 --name redis-service --dry-run=client -o yaml
(This will automatically use the pod's labels as selectors)
Or
kubectl create service clusterip redis --tcp=6379:6379 --dry-run=client -o yaml (This will not use the pods labels as selectors, instead it will assume selectors as app=redis. You cannot pass in selectors as an option. So it does not work very well if your pod has a different label set. So generate the file and modify the selectors before creating the service)
https://github.com/kubernetes/kubernetes/issues/46191

Create a Service named nginx of type NodePort to expose pod nginx's port 80 on port 30080 on the nodes:

kubectl expose pod nginx --type=NodePort --port=80 --name=nginx-service --dry-run=client -o yaml
k expose pod redis --type=ClusterIP --port=6379 --name=redis-service
k expose pod httpd --type=ClusterIP --port=80 --target-port=80 --name=httpd

(This will automatically use the pod's labels as selectors, but you cannot specify the node port. You have to generate a definition file and then add the node port in manually before creating the service with the pod.)

Or

kubectl create service nodeport nginx --tcp=80:80 --node-port=30080 --dry-run=client -o yaml
k create service clusterip httpd --target-port=80 --dry-run=client -o yaml

(This will not use the pods labels as selectors)
Both the above commands have their own challenges. While one of it cannot accept a selector the other cannot accept a node port. I would recommend going with the kubectl expose command. If you need to specify a node port, generate a definition file using the same command and manually input the nodeport before creating the service.
https://github.com/kubernetes/kubernetes/issues/25478


https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands
https://kubernetes.io/docs/reference/kubectl/conventions/

# use imperative commands for creating a pod
k run --help
k run nginx-pod --image=nginx:alpine
k run redis --image=redis:alpine --labels="tier=db"

k create service clusterip --help
k create service clusterip redis-service --target-port=80 --dry-run=client -o yaml
k run custom-nginx --image=nginx --port=8080
k run httpd --image=httpd:alpine --port=80 --expose=true

k apply command ---
the apply command stores last applied changes in an annotation
annotations:
  kubectl.kubernetes.io/last-applied-configuration:
   {"json format of last applied configuration"}